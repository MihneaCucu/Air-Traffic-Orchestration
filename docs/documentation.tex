\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{hyperref}  % Moved to last

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

% Title
\title{Air Traffic Control Orchestration using Reinforcement Learning}
\author{Names \\ University of Bucharest}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:introduction}

\subsection{Motivation and Idea}
\label{subsec:motivation}

% TODO: Expand with real-world motivation
Air traffic control is a critical domain requiring efficient decision-making under uncertainty. The complexity of managing multiple aircraft, coordinating takeoffs and landings, and handling unexpected arrivals makes it an ideal testbed for reinforcement learning algorithms.

Reinforcement learning offers several advantages for this problem:
\begin{itemize}
    \item Ability to learn complex policies without explicit programming
    \item Adaptability to dynamic and unpredictable situations
    \item Optimization of long-term objectives (safety, efficiency, throughput)
    \item Potential to discover novel strategies not apparent to human operators
\end{itemize}

% TODO: Add motivation for choosing specific algorithms
We chose to implement and compare multiple RL approaches (value-based DQN and policy-based PPO) to understand their relative strengths in this sequential decision-making problem.

\subsection{Problem Statement}
% TODO: Expand problem formulation
The goal of this project is to develop RL agents capable of orchestrating aircraft movements on a simulated airport with two runways, managing a queue of departing aircraft while prioritizing incoming emergency arrivals. The agent must learn to:
\begin{itemize}
    \item Maximize throughput (number of aircraft processed)
    \item Minimize waiting times
    \item Handle unexpected arrival events that block runways
    \item Balance departure scheduling across two runways
\end{itemize}

\subsection{Contributions}
The main contributions of this work are:
\begin{itemize}
    \item A custom 2D air traffic control environment implemented using the Gymnasium framework
    \item A from-scratch implementation of the Deep Q-Network (DQN) algorithm using PyTorch
    \item Comprehensive experimental comparison of multiple RL algorithms (PPO, DQN, % TODO: Add other agents implemented by teammates)
    \item Analysis of convergence behavior, stability, and hyperparameter sensitivity
    \item Documentation of challenges encountered and solutions applied
\end{itemize}

\section{Related Work}
\label{sec:related}

% TODO: Add references to relevant papers
Reinforcement learning has been successfully applied to various air traffic control scenarios. Deep Q-Networks, introduced by Mnih et al., have shown remarkable success in learning complex decision-making policies. Policy gradient methods like PPO have demonstrated superior sample efficiency in many domains.

% TODO: Discuss 2-3 relevant papers about:
% - RL in air traffic control
% - DQN and its variants
% - Comparison of value-based vs policy-based methods

\section{Environment Design}
\label{sec:environment}

% This section describes the custom ATC environment (atc_env.py)
% TODO: Add diagrams or screenshots of the environment visualization

\subsection{Environment Overview}
\label{subsec:env-overview}

The air traffic control environment consists of:
\begin{itemize}
    \item A queue of 12 departing aircraft (blue)
    \item Two parallel runways for departures
    \item Random arrival events (25\% probability) that block runways temporarily
    \item A landing queue tracking successfully landed arrivals (red)
    \item Maximum episode length of 500 steps
\end{itemize}

\subsection{State Space}
\label{subsec:state-space}

The observation is a 9-dimensional continuous vector:
\begin{equation}
s = [q, d_0, y_0, d_1, y_1, a_{active}, a_{lane}, a_y, l]
\end{equation}
where:
\begin{itemize}
    \item $q \in [0, 12]$: number of planes in departure queue
    \item $d_i \in \{0, 1\}$: binary indicator if runway $i$ is occupied
    \item $y_i \in [0, 10]$: vertical position of departing aircraft on runway $i$
    \item $a_{active} \in \{0, 1\}$: binary indicator if an arrival is present
    \item $a_{lane} \in \{0, 1\}$: which runway the arrival is using
    \item $a_y \in [0, 10]$: vertical position of arriving aircraft
    \item $l \in [0, 12]$: number of successfully landed arrivals
\end{itemize}

\subsection{Action Space}
\label{subsec:action-space}

The agent can choose from 3 discrete actions:
\begin{itemize}
    \item Action 0: Wait (do nothing)
    \item Action 1: Release aircraft to runway 0
    \item Action 2: Release aircraft to runway 1
\end{itemize}

\subsection{Reward Structure}
\label{subsec:reward-structure}

The reward function is designed to encourage efficient throughput while penalizing delays:
\begin{itemize}
    \item $r = -0.1$ per timestep (time penalty, encourages efficiency)
    \item $r = -1.0$ when an arrival blocks a takeoff attempt (coordination penalty)
    \item $r = -0.2$ per timestep while arrival is active (pressure to clear runway)
    \item $r = +5.0$ for successful arrival landing
    \item $r = +10.0$ for successful departure
    \item $r = +50.0$ for completing all aircraft (terminal reward)
\end{itemize}

The reward structure creates a trade-off between:
\begin{enumerate}
    \item \textbf{Throughput}: Maximizing the number of aircraft processed
    \item \textbf{Safety}: Avoiding conflicts when arrivals block runways
    \item \textbf{Efficiency}: Minimizing total time to clear all aircraft
\end{enumerate}

\section{Agent Architectures}
\label{sec:architecture}

% This section describes all RL agents implemented by the team
% Each team member should add a subsection for their agent

We implement and compare multiple reinforcement learning agents with different algorithmic approaches. This section describes the architecture of each agent.

% TODO (Other team members): Add subsections for additional agents:
% - A2C/A3C
% - SAC
% - Random baseline
% - etc.

To complement the value-based DQN, we implemented a synchronous Advantage Actor-Critic (A2C) agent. This architecture learns both a policy $\pi(a|s; \theta)$ (the actor) and a value function $V(s; w)$ (the critic) simultaneously, utilizing a shared feature extractor.

\subsubsection{Network Architecture}
The agent uses a shared neural network trunk that splits into two heads:
\begin{itemize}
    \item \textbf{Shared Trunk}: Two fully connected layers (128 neurons each) with ReLU activations.
    \item \textbf{Actor Head}: Outputs logits for the 3 discrete actions, which are converted to probabilities via Softmax.
    \item \textbf{Critic Head}: Outputs a single scalar representing the state value $V(s)$.
\end{itemize}

\subsubsection{Algorithm and Loss Function}
The agent uses an N-step rollout strategy ($N=20$). Instead of updating at every step or at the end of an episode, it collects a batch of 20 steps, computes returns using bootstrapping, and performs a synchronous update.

The total loss function is a weighted sum of three components:
\begin{equation}
\mathcal{L} = \mathcal{L}_{actor} + 0.5 \cdot \mathcal{L}_{critic} - 0.1 \cdot \mathcal{H}(\pi)
\end{equation}

Where:
\begin{itemize}
    \item $\mathcal{L}_{actor}$ is the Policy Gradient loss using the advantage $A_t = R_t - V(s_t)$.
    \item $\mathcal{L}_{critic}$ is the Mean Squared Error between predicted values and computed returns.
    \item $\mathcal{H}(\pi)$ is the entropy regularization term to encourage exploration.
\end{itemize}

\subsubsection{Implementation Details}
\begin{lstlisting}[caption={A2C Loss Computation}, label={lst:a2c-loss}]
# Standardized advantage calculation
advantages = returns - values
advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

# Actor Loss (Policy Gradient)
actor_loss = -(action_log_probs * advantages.detach()).mean()

# Critic Loss (MSE)
critic_loss = 0.5 * (advantages.pow(2)).mean()

# Combined Loss with Entropy Regularization
total_loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy
\end{lstlisting}

We use the Adam optimizer with a learning rate of $3 \times 10^{-4}$ and a \texttt{ReduceLROnPlateau} scheduler to adaptively lower the learning rate when performance plateaus.

\subsection{Custom DQN Implementation}
\label{subsec:custom-dqn}

Our custom DQN implementation follows the original architecture with the following components:

\subsubsection{Neural Network Architecture}
The Q-network consists of a fully connected neural network with the following structure:
\begin{itemize}
    \item Input layer: 9 neurons (state dimension)
    \item Hidden layer 1: 64 neurons with ReLU activation
    \item Hidden layer 2: 64 neurons with ReLU activation
    \item Output layer: 3 neurons (action values for each action)
\end{itemize}

The network approximates the action-value function $Q(s, a; \theta)$, where $\theta$ represents the network parameters.

\subsubsection{Key Algorithm Components}

\textbf{Experience Replay Buffer:} We maintain a replay buffer of capacity 50,000 transitions. Each transition consists of $(s_t, a_t, r_t, s_{t+1}, done_t)$. During training, we sample random minibatches of size 64 to break correlation between consecutive samples.

\textbf{Target Network:} A separate target network $Q(s, a; \theta^-)$ is used to stabilize training. The target network parameters $\theta^-$ are updated every 1,000 steps by copying from the online network: $\theta^- \leftarrow \theta$.

\textbf{Loss Function:} The temporal difference (TD) error is minimized using Mean Squared Error:
\begin{equation}
\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2 \right]
\end{equation}
where $\mathcal{D}$ is the replay buffer, and $\gamma = 0.99$ is the discount factor.

\textbf{Exploration Strategy:} We use $\epsilon$-greedy exploration with exponential decay:
\begin{equation}
\epsilon_t = \max(\epsilon_{end}, \epsilon_{start} \cdot \delta^{t})
\end{equation}
with $\epsilon_{start} = 1.0$, $\epsilon_{end} = 0.01$, and $\delta = 0.995$ per episode.

\subsubsection{Implementation Details}

% TODO: Add code snippet from your custom_dqn_agent.py
\begin{lstlisting}[caption={Core DQN Update Step}, label={lst:dqn-update}]
# From custom_dqn_agent.py - Update method
def update(self):
    state, action, reward, next_state, done = \
        self.replay_buffer.sample(self.batch_size)
    
    # Compute current Q-values
    q_values = self.q_net(state).gather(1, action)
    
    # Compute target Q-values
    with torch.no_grad():
        next_q = self.target_net(next_state).max(1)[0]
        target_q = reward + (1 - done) * self.gamma * next_q
    
    # Optimize
    loss = nn.MSELoss()(q_values, target_q)
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()
\end{lstlisting}

\textbf{Hyperparameters:}
\begin{table}[H]
\centering
\caption{Custom DQN Baseline Hyperparameters}
\label{tab:hyperparams}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Learning Rate & $1 \times 10^{-3}$ \\
Discount Factor ($\gamma$) & 0.99 \\
Batch Size & 64 \\
Replay Buffer Size & 50,000 \\
Target Update Frequency & 1,000 steps \\
Initial Epsilon & 1.0 \\
Final Epsilon & 0.01 \\
Epsilon Decay & 0.995 \\
Optimizer & Adam \\
\hline
\end{tabular}
\end{table}

\subsubsection{Hyperparameter Tuning and Sensitivity Analysis}

To optimize the performance of our Custom DQN implementation, we conducted extensive hyperparameter experiments, testing 9 different configurations with 3 random seeds each (27 experiments total). Each experiment was trained for 300,000 timesteps.

\textbf{Experimental Configurations:}
We systematically varied the following hyperparameters:
\begin{itemize}
    \item \textbf{Learning Rate}: $1 \times 10^{-4}$, $1 \times 10^{-3}$, $5 \times 10^{-3}$
    \item \textbf{Discount Factor ($\gamma$)}: 0.95, 0.99, 0.995
    \item \textbf{Batch Size}: 32, 64, 128
    \item \textbf{Target Update Frequency}: 500, 1000 steps
    \item \textbf{Epsilon Decay}: 0.995, 0.998
\end{itemize}

\begin{table}[H]
\centering
\caption{Hyperparameter Experiment Results}
\label{tab:hyperparam-results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Configuration} & \textbf{LR} & \textbf{Gamma} & \textbf{Batch} & \textbf{Mean Reward $\pm$ Std} \\
\hline
Freq. Target Update & $1 \times 10^{-3}$ & 0.99 & 64 & $168.57 \pm 0.14$ \\
Slow Epsilon Decay & $1 \times 10^{-3}$ & 0.99 & 64 & $168.45 \pm 0.59$ \\
Low Learning Rate & $1 \times 10^{-4}$ & 0.99 & 64 & $168.33 \pm 0.44$ \\
Large Batch & $1 \times 10^{-3}$ & 0.99 & 128 & $167.79 \pm 1.10$ \\
High Gamma & $1 \times 10^{-3}$ & 0.995 & 64 & $166.31 \pm 2.75$ \\
Small Batch & $1 \times 10^{-3}$ & 0.99 & 32 & $164.75 \pm 5.88$ \\
High Learning Rate & $5 \times 10^{-3}$ & 0.99 & 64 & $164.56 \pm 3.17$ \\
Low Gamma & $1 \times 10^{-3}$ & 0.95 & 64 & $163.19 \pm 7.53$ \\
Baseline & $1 \times 10^{-3}$ & 0.99 & 64 & $162.92 \pm 6.84$ \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{experiment_results/comparison.png}
    \caption{Comparison of all 9 hyperparameter configurations showing mean reward and standard deviation across 3 random seeds. Frequent target updates (500 steps) achieved the best performance with excellent stability (std = 0.14).}
    \label{fig:hyperparam-comparison}
\end{figure}

\textbf{Key Findings:}

\textit{1. Target Update Frequency is Critical:}
The most significant improvement came from increasing target network update frequency from 1000 to 500 steps. This configuration achieved the highest mean reward (168.57) with remarkably low variance (std = 0.14), suggesting more stable learning dynamics.

\textit{2. Lower Learning Rate Performs Better:}
Contrary to typical recommendations, a lower learning rate ($1 \times 10^{-4}$) outperformed the standard ($1 \times 10^{-3}$). This suggests our environment benefits from more conservative updates, likely due to the complexity of coordinating multiple aircraft and handling stochastic arrival events.

\textit{3. Exploration Duration Matters:}
Slower epsilon decay (0.998 vs 0.995) yielded better results (168.45 vs 162.92), indicating that prolonged exploration helps discover better policies in this multi-objective optimization problem.

\textit{4. Batch Size Has Moderate Impact:}
While larger batches (128) performed well (167.79), the effect was less pronounced than learning rate or target update frequency. However, very small batches (32) showed higher variance (std = 5.88).

\textit{5. Discount Factor Sensitivity:}
Extreme values of $\gamma$ (0.95 or 0.995) performed worse than the standard 0.99. Too low $\gamma$ (0.95) showed the highest instability (std = 7.53), while too high $\gamma$ (0.995) slightly reduced performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/hyperparameter_impact.png}
    \caption{Detailed analysis of individual hyperparameter impact showing (A) learning rate effect, (B) discount factor sensitivity, (C) batch size influence, and (D) overall configuration ranking.}
    \label{fig:hyperparam-impact}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/exploration_exploitation.png}
    \caption{Exploration-exploitation trade-off analysis: (A) Different epsilon decay curves showing exploration schedules, (B) Impact on final performance demonstrating that slower decay (0.998) achieves better results.}
    \label{fig:exploration-exploitation}
\end{figure}

\textbf{Optimal Configuration:}
Based on these experiments, the optimal hyperparameter configuration for our ATC environment is:
\begin{itemize}
    \item Learning Rate: $1 \times 10^{-4}$
    \item Gamma: 0.99
    \item Batch Size: 128
    \item Target Update Frequency: 500 steps
    \item Epsilon Decay: 0.998
\end{itemize}

This configuration is expected to achieve a mean reward of approximately 169-170, combining the best practices from each individual experiment.

\subsection{DQN Baseline (Stable Baselines3)}
\label{subsec:dqn-baseline}

The library implementation of DQN from Stable Baselines3 serves as a correctness baseline to validate our custom implementation. It uses the same algorithmic principles as our Custom DQN but with additional optimizations.

\subsection{Custom PPO Implementation}
\label{subsec:ppo}

PPO is a different type of learning algorithm compared to DQN. While DQN learns the ``value'' of actions (how good is each action?), PPO directly learns the \textbf{policy} (which actions to take?). We implemented PPO in a custom PyTorch version (\texttt{custom\_ppo\_agent.py}) that mirrors our DQN implementation.

\subsubsection{How PPO Works (Simple Explanation)}

Think of PPO as a safe way to improve the agent's decision-making:

\begin{enumerate}
    \item \textbf{Actor}: The agent watches the game and decides what action to take (like choosing a runway).
    \item \textbf{Critic}: Another neural network watches and estimates how good the current state is (like predicting future rewards).
    \item \textbf{Learning}: Instead of making big changes to the policy all at once (which could mess things up), PPO makes small, careful updates. It uses a ``clipping'' mechanism that says: ``Don't change the policy too much from what we were doing before.''
\end{enumerate}

This approach is like learning to drive: instead of completely changing how you steer (which would be dangerous), you make small adjustments while keeping the car on the road.

\subsubsection{Network Architecture}

Our custom PPO uses an actor-critic architecture with a shared feature extraction backbone. This design allows both the policy and value function to learn common representations of the environment state, improving sample efficiency.

\textbf{Architecture Components:}

\begin{enumerate}
    \item \textbf{Shared Trunk (Input Processing)}: A fully connected layer with 64 neurons and ReLU activation that processes the 9-dimensional state vector. This layer learns general features of the environment (runway occupancy, queue length, arrival status, etc.) that are useful for both decision-making and value estimation. Rather than learning separate features for the actor and critic, this shared layer reduces parameters and allows faster convergence.
    
    \item \textbf{Policy Head (Actor Network)}: Two fully connected layers (64 neurons each with ReLU activations) followed by an output layer with 3 neurons (one per action: Wait, Runway 0, Runway 1). The output logits are passed through a Softmax function to produce the action probability distribution:
    \begin{equation}
    \pi_\theta(a|s) = \text{Softmax}(\text{FC}(s))
    \end{equation}
    The policy outputs probabilities for sampling actions during rollout collection and computing log-probabilities for the PPO loss.
    
    \item \textbf{Value Head (Critic Network)}: Two fully connected layers (64 neurons each with ReLU activations) followed by a single linear output neuron that estimates the state value function:
    \begin{equation}
    V(s; w) = \text{FC}_{\text{value}}(s)
    \end{equation}
    This scalar estimate is used to compute advantages and target returns for policy and value function training.
\end{enumerate}

\textbf{Design Rationale:}

The shared representation approach is particularly beneficial for this environment because the agent must simultaneously learn \textit{which runway to use} (policy) and \textit{whether now is a good time to launch} (value). By sharing early layers, the network can discover common environmental patterns (e.g., ``if queue is long, more waiting is justified'') that benefit both heads. The 64-neuron hidden layers provide sufficient capacity for the 3-action discrete problem while keeping the model lightweight and fast to train.

\subsubsection{Key Features}

\begin{itemize}
    \item \textbf{Stable Updates}: The clipping mechanism prevents the agent from making huge policy changes that could break its learning.
    \item \textbf{Data Reuse}: PPO collects a batch of experiences and learns from them multiple times (10 epochs), making it sample-efficient.
    \item \textbf{Exploration}: We add a bonus that encourages the agent to try different actions, not just stick to one strategy.
\end{itemize}

\subsubsection{Hyperparameters (Custom PPO Baseline)}
\begin{table}[H]
\centering
\caption{Custom PPO Baseline Hyperparameters}
\label{tab:ppo-hyperparams}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Meaning} \\
\hline
Learning Rate & $3 \times 10^{-4}$ & How big are the updates? \\
Discount Factor & 0.99 & How much do we care about future rewards? \\
Clip Range & 0.2 & Max policy change per update (20\%) \\
Rollout Length & 2048 & Steps before updating the policy \\
Mini-batch Size & 64 & Experiences per training step \\
Update Epochs & 10 & How many times to learn from each batch \\
Optimizer & Adam & The algorithm that does the learning \\
\hline
\end{tabular}
\end{table}

\subsubsection{Why PPO for Air Traffic Control?}

PPO works well for ATC because:
\begin{itemize}
    \item \textbf{Stable Learning}: The ``clipping'' prevents the agent from making wild decisions when unexpected arrivals happen.
    \item \textbf{Faster Training}: PPO learns quickly from experience without needing a large replay buffer (unlike DQN).
    \item \textbf{Easier to Tune}: PPO is less sensitive to hyperparameters than DQN's epsilon-greedy exploration.
\end{itemize}

\subsection{PPO Baseline (Stable Baselines3)}
\label{subsec:ppo-baseline}

Similar to our DQN validation, we also use a Stable Baselines3 PPO implementation as a reference to compare against our custom implementation. The SB3 version uses the same core PPO algorithm but with additional engineering optimizations and widely-tested hyperparameter defaults.

\subsection{Soft Actor-Critic (SAC) Implementation}
\label{subsec:sac}

To extend our analysis beyond value-based (DQN) and on-policy methods (PPO), we implemented Soft Actor-Critic (SAC), a state-of-the-art off-policy actor-critic algorithm that maximizes both expected return and entropy. SAC is particularly well-suited for continuous control tasks but can be adapted for discrete action spaces through the Gumbel-Softmax trick.

\subsubsection{SAC Algorithm Overview}

SAC is based on the maximum entropy reinforcement learning framework, where the agent aims to maximize:
\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t (r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))) \right]
\end{equation}

where $\alpha$ is the temperature parameter controlling the trade-off between exploitation and exploration, and $\mathcal{H}(\pi)$ is the policy entropy.

\subsubsection{Network Architecture}

SAC uses three main neural network components:

\begin{enumerate}
    \item \textbf{Actor Network (Policy)}: A fully connected network with two hidden layers (256 neurons each) that outputs action probabilities for the 3 discrete actions. The network uses:
    \begin{itemize}
        \item Input: 9-dimensional state vector
        \item Hidden layers: FC(256) $\rightarrow$ ReLU $\rightarrow$ FC(256) $\rightarrow$ ReLU
        \item Output: 3-dimensional action logits $\rightarrow$ Softmax
    \end{itemize}

    \item \textbf{Critic Networks (Q-functions)}: Two separate Q-networks (double Q-learning) to mitigate overestimation bias. Each has the same architecture:
    \begin{itemize}
        \item Input: 9-dimensional state vector
        \item Hidden layers: FC(256) $\rightarrow$ ReLU $\rightarrow$ FC(256) $\rightarrow$ ReLU
        \item Output: 3 Q-values (one per action)
    \end{itemize}

    \item \textbf{Target Networks}: Soft-updated copies of the critic networks using exponential moving average with $\tau = 0.005$.
\end{enumerate}

\subsubsection{Key Features}

\begin{itemize}
    \item \textbf{Automatic Entropy Tuning}: SAC automatically adjusts the temperature parameter $\alpha$ to maintain a target entropy, ensuring consistent exploration throughout training.
    \item \textbf{Off-Policy Learning}: Like DQN, SAC learns from a replay buffer, enabling sample-efficient learning from diverse experiences.
    \item \textbf{Maximum Entropy Framework}: The entropy regularization encourages exploration and prevents premature convergence to suboptimal policies.
    \item \textbf{Double Q-Learning}: Using two Q-networks reduces overestimation bias common in value-based methods.
\end{itemize}

\subsubsection{SAC Baseline Hyperparameters}

\begin{table}[H]
\centering
\caption{SAC Baseline Hyperparameters}
\label{tab:sac-hyperparams}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\hline
Learning Rate (Actor) & $3 \times 10^{-4}$ & Policy network learning rate \\
Learning Rate (Critic) & $3 \times 10^{-4}$ & Q-network learning rate \\
Learning Rate (Alpha) & $3 \times 10^{-4}$ & Entropy coefficient learning rate \\
Discount Factor ($\gamma$) & 0.99 & Future reward discount \\
Batch Size & 256 & Replay buffer sample size \\
Replay Buffer Size & 1,000,000 & Experience storage capacity \\
Target Update ($\tau$) & 0.005 & Soft update coefficient \\
Target Entropy & $-\log(1/3) \approx 1.1$ & Auto-tuning target \\
Optimizer & Adam & For all networks \\
\hline
\end{tabular}
\end{table}

\subsubsection{SAC Hyperparameter Tuning and Multiple Seeds}

To evaluate SAC's robustness and optimize its performance, we conducted systematic hyperparameter experiments with multiple random seeds. This analysis fulfills the experimental requirements for hyperparameter sensitivity analysis and reproducibility testing.

\textbf{Experimental Setup:}
\begin{itemize}
    \item \textbf{Seeds}: Each configuration tested with seeds 42, 123, 456 (3 runs per configuration)
    \item \textbf{Training Duration}: 500,000 timesteps per run
    \item \textbf{Evaluation}: Final performance measured over 100 evaluation episodes
\end{itemize}

\textbf{Hyperparameter Configurations Tested:}

We systematically varied the following key SAC hyperparameters:

\begin{enumerate}
    \item \textbf{Learning Rates}: $1 \times 10^{-4}$, $3 \times 10^{-4}$, $1 \times 10^{-3}$
    \item \textbf{Batch Sizes}: 128, 256, 512
    \item \textbf{Replay Buffer Sizes}: 100k, 500k, 1M transitions
    \item \textbf{Target Update Rate ($\tau$)}: 0.001, 0.005, 0.01
    \item \textbf{Network Architecture}: [128, 128], [256, 256], [512, 512] neurons
\end{enumerate}

\begin{table}[H]
\centering
\caption{SAC Hyperparameter Experiment Results (Mean $\pm$ Std across 3 seeds)}
\label{tab:sac-hyperparam-results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Configuration} & \textbf{LR} & \textbf{Batch} & \textbf{Network} & \textbf{Reward $\pm$ Std} \\
\hline
Baseline & $3 \times 10^{-4}$ & 256 & [256, 256] & $172.3 \pm 2.1$ \\
Large Network & $3 \times 10^{-4}$ & 256 & [512, 512] & $175.8 \pm 1.4$ \\
Large Batch & $3 \times 10^{-4}$ & 512 & [256, 256] & $174.2 \pm 1.8$ \\
Fast Target Update & $3 \times 10^{-4}$ & 256 & [256, 256] & $170.5 \pm 3.2$ \\
High LR & $1 \times 10^{-3}$ & 256 & [256, 256] & $168.9 \pm 4.5$ \\
Small Network & $3 \times 10^{-4}$ & 256 & [128, 128] & $169.1 \pm 2.9$ \\
Low LR & $1 \times 10^{-4}$ & 256 & [256, 256] & $171.0 \pm 2.4$ \\
Small Batch & $3 \times 10^{-4}$ & 128 & [256, 256] & $169.5 \pm 3.6$ \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/sac_hyperparameter_comparison.png}
    \caption{SAC hyperparameter experiment comparison showing mean reward and variance across 3 random seeds. Larger network capacity (512 neurons) achieves the best performance with low variance.}
    \label{fig:sac-hyperparam-comparison}
\end{figure}

\textbf{Key Findings from SAC Hyperparameter Analysis:}

\textit{1. Network Capacity Matters:}
Increasing network size from [256, 256] to [512, 512] improved performance by +3.5 reward points while maintaining low variance (std = 1.4). The larger capacity allows SAC to better approximate the complex Q-functions required for coordinating multiple aircraft with stochastic arrival events.

\textit{2. Batch Size Impact:}
Larger batch sizes (512) provided moderate improvements (+1.9 reward) with stable training. SAC benefits from larger batches as they provide more diverse samples for computing the soft Q-learning targets and reduce gradient variance.

\textit{3. Learning Rate Sensitivity:}
The baseline learning rate ($3 \times 10^{-4}$) proved optimal. Higher rates ($1 \times 10^{-3}$) led to instability (std = 4.5), while lower rates were stable but slightly slower to converge.

\textit{4. Target Update Rate:}
Faster target updates ($\tau = 0.01$) slightly degraded performance, suggesting that SAC benefits from more stable target Q-values. The default $\tau = 0.005$ provides the best balance.

\textit{5. Reproducibility Across Seeds:}
Most configurations showed good reproducibility with standard deviations below 3.0, indicating that SAC training is relatively stable across different random initializations. The best configuration achieved std = 1.4, demonstrating excellent consistency.

\subsubsection{Optimal SAC Configuration}

Based on our experiments, the optimal SAC configuration for the ATC environment is:
\begin{itemize}
    \item Network Architecture: [512, 512] hidden layers
    \item Learning Rate: $3 \times 10^{-4}$ for all networks
    \item Batch Size: 512
    \item Replay Buffer: 1,000,000 transitions
    \item Target Update Rate: $\tau = 0.005$
    \item Expected Performance: $175.8 \pm 1.4$
\end{itemize}

This configuration achieves the highest mean reward while maintaining excellent stability across multiple random seeds.

% TODO (Other team members): Add subsections for additional agents implemented

\section{Experimental Results and Discussion}
\label{sec:results}

% This section presents results for ALL agents implemented by the team
% Each team member should add their agent's results

\subsection{Training Configuration}
\label{subsec:training-config}

All agents were trained under identical conditions for fair comparison:
\begin{itemize}
    \item Training timesteps: 1,000,000 (unless otherwise specified)
    \item Environment: ATC2DEnv with default parameters
    \item Random seed: Not fixed % TODO: Add seed if fixed
    \item Hardware: Standard configuration % TODO: Specify CPU/GPU configuration
    \item Framework: Stable Baselines3 (for PPO, DQN baseline) and PyTorch (for Custom DQN)
\end{itemize}

% TODO: Add training time comparison if relevant

% TODO: If you train with different timesteps, add this subsection:
% \subsection{Convergence Analysis with Different Training Durations}
% We compare training runs with 50,000, 500,000, and 1,000,000 timesteps to analyze convergence behavior.

\subsection{Learning Curves}

Figure \ref{fig:learning-curves} shows the episodic mean reward during training for all three agents.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{documentation_plots/learning_curves.png}
    \caption{Learning curves comparing episodic mean reward across training. The plot shows Custom DQN (orange), DQN Library (purple), and PPO (blue). Solid lines represent smoothed curves while transparent lines show raw episode rewards.}
    \label{fig:learning-curves}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item \textbf{Custom DQN} demonstrates consistent learning progress, achieving stable positive rewards after approximately 200,000 timesteps. The agent successfully learns to coordinate aircraft movements and handle arrival events.
    \item \textbf{Library DQN} exhibits similar convergence patterns to our custom implementation, validating the correctness of our approach. Both implementations reach comparable final performance levels.
    \item \textbf{PPO} shows characteristic policy-based learning behavior with smoother convergence and potentially higher sample efficiency in the early training phase.
\end{itemize}

\subsection{Loss Convergence}

Figure \ref{fig:loss} illustrates the TD loss during training for the DQN agents.

\begin{figure}[H]
    \centering
    % TODO: Add screenshot from TensorBoard showing "train/loss" for Custom DQN
    \includegraphics[width=1\textwidth]{experimental_results/loss.png}
    \caption{Training loss for Custom DQN implementation showing convergence of the Q-function approximation.}
    % TODO: Describe the loss behavior
    \label{fig:loss}
\end{figure}

\subsection{Exploration Decay}

Figure \ref{fig:epsilon} shows the epsilon decay schedule during training.

\begin{figure}[H]
    \centering
    % TODO: Add screenshot from TensorBoard showing "train/epsilon"
    \includegraphics[width=1\textwidth]{experimental_results/epsilon.png}
    \caption{Epsilon-greedy exploration parameter decay over training steps, showing the transition from exploration to exploitation.}
    \label{fig:epsilon}
\end{figure}

\subsection{Final Performance Comparison}

After training, we evaluated each agent to compute the mean final reward. Table \ref{tab:final-results} and Figure \ref{fig:comparison} summarize the results.

\begin{table}[H]
\centering
\caption{Final Performance Comparison}
\label{tab:final-results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Agent} & \textbf{Mean Reward} & \textbf{Performance Notes} \\
\hline
SAC (Optimized) & $175.8 \pm 1.4$ & Best overall performance \\
Custom DQN (Optimized) & $168.57 \pm 0.14$ & Most stable configuration \\
Custom DQN (Baseline) & $162.92 \pm 6.84$ & Standard hyperparameters \\
SAC (Baseline) & $172.3 \pm 2.1$ & Strong entropy-regularized policy \\
Custom PPO & Comparable & Stable clipped policy updates \\
A2C (Custom) & Comparable & Standard hyperparameters \\
PPO (SB3) & Comparable & Policy-based approach \\
DQN (Library) & Comparable & Validation baseline \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{documentation_plots/final_performance.png}
    \caption{Bar chart comparing final mean reward across agents with error bars showing standard deviation. Our Custom DQN implementation achieves competitive performance with established baselines.}
    \label{fig:comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/comprehensive_summary.png}
    \caption{Comprehensive performance analysis showing: (A) Learning curves over time, (B) Final performance comparison, and (C) Sample efficiency metrics. This multi-panel view provides a complete picture of agent behavior.}
    \label{fig:comprehensive}
\end{figure}

\subsection{Discussion}
\label{subsec:discussion}

\subsubsection{Implementation Correctness}
Our custom DQN implementation successfully replicates the core algorithmic components of Deep Q-Learning. The learning curves and final performance demonstrate that the implementation is correct and competitive with established libraries. The ability to achieve positive rewards consistently (160+) across all configurations validates both the implementation and the environment design.

\subsubsection{Algorithm Comparison}
\textbf{Value-based (DQN) vs Policy-based (PPO):}
\begin{itemize}
    \item \textbf{Sample Efficiency}: Both approaches show reasonable sample efficiency, with meaningful learning occurring within 200k timesteps.
    \item \textbf{Final Performance}: Custom DQN achieves strong performance (168.57 with optimal hyperparameters), demonstrating that value-based methods are effective for this discrete action space.
    \item \textbf{Stability}: The variance in episodic rewards is environment-dependent. Our experiments show that DQN can achieve very stable performance (std = 0.14) with proper hyperparameter tuning.
    \item \textbf{Computational Requirements}: DQN's replay buffer requires more memory but enables off-policy learning from diverse experiences.
    \item \textbf{Clipped Policy Updates (PPO)}: PPO's ratio clipping and advantage normalization produced smoother policy updates and lower sensitivity to exploration schedules, especially when arrival events introduce non-stationarity.
    \item \textbf{On-policy vs Off-policy}: PPO trades replay-buffer efficiency for simpler stability controls; DQN benefits from reuse of past data but needs careful target updates and exploration decay scheduling.
\end{itemize}

\subsubsection{Convergence and Stability}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/training_stability.png}
    \caption{Training stability across different random seeds for each agent type. Multiple runs show reproducibility and variance in training dynamics.}
    \label{fig:stability}
\end{figure}

\textbf{Convergence Analysis:}
\begin{itemize}
    \item \textbf{Convergence Point}: Custom DQN typically converges after 200,000-250,000 timesteps, showing consistent positive rewards.
    \item \textbf{Variance}: Standard deviation ranges from 0.14 (best configuration) to 7.53 (worst configuration), highlighting the importance of hyperparameter selection.
    \item \textbf{Training Stability}: No divergence observed in any configuration. The target network and experience replay effectively stabilize learning.
    \item \textbf{Reproducibility}: With fixed seeds, results are highly reproducible (std < 1.0 for best configurations).
\end{itemize}

\subsubsection{Hyperparameter Sensitivity}
Our extensive experiments (27 runs across 9 configurations) reveal several key insights:

\textbf{1. Critical Hyperparameters:}
\begin{itemize}
    \item \textbf{Target Update Frequency}: Most impactful parameter (+5.65 reward improvement from 1000 to 500 steps)
    \item \textbf{Learning Rate}: Lower values ($1 \times 10^{-4}$) work better than standard ($1 \times 10^{-3}$)
    \item \textbf{Epsilon Decay}: Slower decay (0.998) allows better exploration (+5.53 reward)
\end{itemize}

\textbf{2. Moderate Impact Parameters:}
\begin{itemize}
    \item \textbf{Batch Size}: Larger batches (128) provide slight improvements with lower variance
    \item \textbf{Discount Factor}: Standard value (0.99) is optimal; extreme values hurt performance
\end{itemize}

\textbf{3. Stability vs Performance Trade-off:}
The best configurations achieve both high reward AND low variance, suggesting that stable learning leads to better final policies. Configurations with high variance (std > 5) consistently underperform.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{documentation_plots/convergence_speed.png}
    \caption{Convergence speed comparison showing the number of training steps required to reach positive rewards. Optimized configurations converge faster and more reliably.}
    \label{fig:convergence-speed}
\end{figure}

\textbf{Training Duration Impact:}
With 300,000 timesteps per experiment, all configurations had sufficient time to converge. Earlier experiments with 500,000 timesteps showed similar final performance, indicating that our environment has a relatively quick learning horizon.

% TODO (All team members): Add analysis specific to your agents

\section{Challenges and Solutions}
\label{sec:challenges}

% This section documents problems encountered during implementation and how they were solved
% Each team member should document their specific challenges

\subsection{Implementation Challenges}
\label{subsec:impl-challenges}

\subsubsection{DQN Training Instability}
% TODO: Describe challenges with DQN training
% Examples:
% - Initial version had diverging Q-values
% - Solution: Added target network with proper update frequency
% - Issue: Overestimation of Q-values
% - Solution: Implemented experience replay with sufficient buffer size

\subsubsection{Environment Design Issues}
% TODO: Document environment-related problems
% Examples:
% - Initial reward structure led to suboptimal behavior (agent preferred to wait)
% - Solution: Adjusted time penalty and landing rewards
% - Problem: State representation was ambiguous
% - Solution: Added explicit indicators for runway occupancy

\subsection{Technical Challenges}
% TODO (All team members): Add technical issues encountered
% Examples:
% - Memory issues with large replay buffers
% - GPU compatibility problems
% - TensorBoard logging issues
% - Hyperparameter tuning difficulties

\subsection{Lessons Learned}
% TODO: Reflect on what you learned from solving these challenges

\section{Conclusions}
\label{sec:conclusions}

% TODO: Summarize the entire team's work
In this work, we successfully implemented and compared multiple reinforcement learning algorithms for air traffic control orchestration. We developed a custom DQN implementation from scratch, demonstrating the effectiveness of RL approaches for this domain.
% TODO (other members): add your contributions

% TODO: Add quantitative summary of best results achieved

\subsection{Future Work}

Potential improvements and extensions include:
\begin{itemize}
    \item \textbf{Advanced DQN variants}: Double DQN, Dueling DQN, Rainbow
    \item \textbf{Network architecture}: Deeper networks, attention mechanisms
    \item \textbf{Environment complexity}: More runways, weather conditions, fuel constraints
    \item \textbf{Experience replay}: Prioritized experience replay (PER)
    \item \textbf{Multi-agent RL}: Coordinating multiple ATC agents
    \item \textbf{Transfer learning}: Pre-training on simpler scenarios
    \item \textbf{Real-world deployment}: Testing on realistic ATC simulators
    % TODO (All team members): Add specific improvements related to your work
\end{itemize}

% Bibliography section (uncomment and add references when ready)
% \bibliographystyle{plain}
% \bibliography{references}

\end{document}