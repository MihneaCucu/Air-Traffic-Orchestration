\documentclass[12pt,a4paper]{article}

% Pachete
\usepackage[utf8]{inputenc}
\usepackage[romanian]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{hyperref}
\usepackage[margin=2.5cm]{geometry}

% Stil pentru cod
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\title{Control Trafic Aerian cu Reinforcement Learning}
\author{
    Izabela Jilavu, Mihnea Cucu, Antonio Soare, Cezar Tulceanu, Cristina Cârstea \\
    Universitatea din București
}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introducere}

\subsection{Motivație și Context}

Controlul traficului aerian reprezintă o problemă complexă de optimizare secvențială sub incertitudine, necesitând coordonarea eficientă a mai multor entități dinamice (avioane) într-un spațiu restricționat (piste) cu constrângeri temporale stricte (ferestre de decolare/aterizare) și evenimente stochastice (sosiri neplanificate). Această complexitate inherentă face din domeniul ATC un candidat ideal pentru aplicarea tehnicilor de Reinforcement Learning.

Abordarea tradițională bazată pe reguli predefinite și proceduri manuale prezintă limitări semnificative în situații dinamice și neprevăzute. Reinforcement Learning oferă o alternativă promițătoare prin capacitatea sa de a învăța politici optime direct din interacțiunea cu mediul, fără a necesita specificarea explicită a strategiilor de control.

\textbf{Formularea problemei ca MDP:} Modelăm problema ca un Markov Decision Process (MDP) definit de tuplul $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$ unde:
\begin{itemize}
    \item $\mathcal{S}$ - spațiul stărilor (observații 9-dimensionale)
    \item $\mathcal{A}$ - spațiul acțiunilor (3 acțiuni discrete)
    \item $\mathcal{P}$ - funcția de tranziție (determinată de dinamica mediului)
    \item $\mathcal{R}$ - funcția de reward (proiectată pentru optimizarea multi-obiectiv)
    \item $\gamma = 0.99$ - factorul de discount pentru reward-uri viitoare
\end{itemize}

Obiectivul agenților este să învețe o politică optimă $\pi^*$ care maximizează reward-ul cumulativ așteptat:
\begin{equation}
\pi^* = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]
\end{equation}

\subsection{Ce am construit?}

\textbf{1. Mediul (Environment):}
\begin{itemize}
    \item Un aeroport 2D cu 2 piste paralele
    \item O coadă de 12 avioane care vor să decoleze
    \item Avioane care apar random și vor să aterizeze (și blochează pistele!)
    \item Vizualizare grafică cu pygame
\end{itemize}

\textbf{2. Agenții (cei care învață):}
\begin{itemize}
    \item \textbf{Random Agent} - alege acțiuni la întâmplare (baseline)
    \item \textbf{DQN} - învață valorile acțiunilor (value-based)
    \item \textbf{PPO} - învață direct ce politică să urmeze (policy-based)
    \item \textbf{A2C} - actor-critic sincron
    \item \textbf{SAC} - soft actor-critic cu entropie maximă
\end{itemize}

\textbf{3. Experimente comprehensive:}
\begin{itemize}
    \item Comparații între toți agenții
    \item Teste cu hiperparametri diferiți (50+ configurații)
    \item Analize statistice cu 3 seed-uri diferite
    \item 10 grafice pentru documentație
\end{itemize}

\section{Mediul de Simulare}

\subsection{Arhitectura Mediului}

Am implementat un mediu de simulare 2D conform specificațiilor Gymnasium (successorul OpenAI Gym), reprezentând un aeroport cu capacitate limitată:

\textbf{Componente spațiale:}
\begin{itemize}
    \item \textbf{Zonă de așteptare}: Coadă FIFO cu capacitate maximă de 12 avioane în pregătire pentru decolare
    \item \textbf{Sistemul de piste}: 2 piste paralele independente, fiecare cu lungime de 10 unități spațiale
    \item \textbf{Coridor de apropiere}: Zonă dedicată pentru avioane în procedură de aterizare
    \item \textbf{Zonă de finalizare}: Contorizează avioanele care au aterizat cu succes
\end{itemize}

\textbf{Dinamica temporală și evenimente stochastice:}

Mediul introduce non-determinism prin evenimente de sosire aleatoare. Cu probabilitate $p = 0.25$ la fiecare pas temporal, un avion nou intră în procedură de aterizare, selectând aleator una din cele două piste. Acest mecanism simulează traficul aerian imprevizibil și forțează agentul să dezvolte strategii robuste de coordonare.

\textbf{Condiții de conflict:} Un conflict apare când agentul încearcă să inițieze o decolare pe o pistă ocupată de un avion în procedură de aterizare. Această constrângere de siguranță reprezintă o componentă critică a problemei, penalizând sever deciziile care ar putea genera coliziuni.

\subsection{Condiții Dinamice și Complexitate Realistă}

Pentru a simula condiții aeroportuare realiste, mediul implementează mai multe mecanisme stochastice suplimentare:

\textbf{1. Condiții Meteorologice Variabile:}
\begin{itemize}
    \item \textbf{Viteza vântului}: Parametru dinamic $w \in [0, 0.6]$ care variază aleator cu $\Delta w \sim \mathcal{U}(-0.05, 0.05)$ la fiecare pas
    \item \textbf{Impact asupra vitezei}: Avioanele în aer se deplasează cu viteză redusă: $v_{eff} = 1.0 - 0.5 \cdot w$
    \item \textbf{Efecte vizuale}: Când $w > 0.3$, sistemul de rendering simulează turbulențe prin animații de shake
\end{itemize}

\textbf{2. Urgențe de Combustibil:}
\begin{itemize}
    \item Fiecare avion din coadă are un timer de combustibil: $f_i \sim \mathcal{U}(20, 100)$ pași
    \item Timer-ul se decrementează la fiecare pas: $f_i \leftarrow f_i - 1$
    \item Când $f_i < 5$, se declanșează urgență de combustibil (penalty crescut)
    \item La fiecare decolare reușită, avionul nou primit primește combustibil fresh
\end{itemize}

\textbf{3. Near-Misses și Safety Score:}
\begin{itemize}
    \item Mediul urmărește numărul de "near-misses" (situații periculoase evitate la limită)
    \item Safety score inițial: 100, se reduce cu fiecare conflict sau urgență
    \item Feedback vizual: alertă roșie intermitentă când safety score $< 70$
\end{itemize}

Aceste elemente transformă problema dintr-o simplă coordonare statică într-un proces de decizie sub incertitudine cu constrângeri temporale multiple și trade-off-uri complexe între siguranță și throughput.

\subsection{Ce informații primește agentul?}

Agentul vede 9 numere care descriu starea curentă:
\begin{enumerate}
    \item Câte avioane mai sunt în coadă (0-12)
    \item E ocupată pista 0? (0/1)
    \item Unde e avionul pe pista 0? (0-10)
    \item E ocupată pista 1? (0/1)
    \item Unde e avionul pe pista 1? (0-10)
    \item Vine vreun avion să aterizeze? (0/1)
    \item Pe ce pistă vine? (0/1)
    \item Unde e avionul care aterizează? (0-10)
    \item Câte avioane au aterizat cu succes? (0-12)
\end{enumerate}

\textbf{Observație importantă:} Starea observabilă NU include direct viteza vântului sau timer-ele de combustibil. Agentul trebuie să învețe să infereze urgențele din penalitățile de reward și să dezvolte strategii robuste care funcționează sub condiții variate, fără acces complet la starea internă a mediului. Aceasta reprezintă o problemă de \textbf{partial observability}, crescând dificultatea învățării.

\subsection{Ce poate face agentul?}

Are 3 acțiuni posibile:
\begin{itemize}
    \item \textbf{Acțiunea 0:} Așteaptă (nu face nimic)
    \item \textbf{Acțiunea 1:} Trimite un avion pe pista 0
    \item \textbf{Acțiunea 2:} Trimite un avion pe pista 1
\end{itemize}

\subsection{Funcția de Reward - Design și Justificare}

\subsubsection{Structura Multi-Obiectiv}

Am proiectat o funcție de reward compozită care echilibrează trei obiective concurente:

\textbf{1. Time Penalty (Eficiență):}
\begin{equation}
r_{time} = -0.01 \quad \forall t
\end{equation}
Penalizare constantă per pas temporal care previne strategii de "inacțiune infinită". Încurajează finalizarea rapidă a task-ului.

\textbf{2. Task Completion Rewards (Throughput):}
\begin{align}
r_{departure} &= +15.0 \quad \text{(decolare reușită)} \\
r_{landing} &= +5.0 \quad \text{(aterizare facilitată)}
\end{align}
Reward-uri pozitive pentru progres măsurabil în procesarea avioanelor.

\textbf{3. Safety Penalties (Constrângeri):}
\begin{align}
r_{occupied} &= -5.0 \quad \text{(încercare pe pistă ocupată)} \\
r_{conflict} &= -10.0 \quad \text{(conflict cu arrival)}
\end{align}
Penalizări pentru violări de siguranță, scaling-ul asimetric ($|r_{conflict}| > |r_{occupied}|$) reflectă severitatea relativă.

\textbf{4. Terminal Bonus (Completion):}
\begin{equation}
r_{terminal} = +100.0 \quad \text{(toate avioanele procesate)}
\end{equation}
Reward sparse la sfârșitul episodului pentru finalizare completă.

\subsubsection{Analiza Trade-off-urilor}

Funcția de reward creează următoarele tensiuni strategice:

\begin{enumerate}
    \item \textbf{Viteză vs Siguranță}: Time penalty $(-0.01/step)$ împinge spre acțiune rapidă, dar penalitățile de conflict $(-10)$ impun prudență. Raportul $\frac{10}{0.01} = 1000$ pași înseamnă că un singur conflict echivalează cu 1000 pași de așteptare.

    \item \textbf{Throughput vs Coordonare}: Reward-ul pentru departure $(+15)$ trebuie maximizat, dar apariția stochastică a arrival-urilor forțează așteptare strategică. Agentul învață când să "sacrifice" un pas pentru a evita conflicte viitoare.

    \item \textbf{Reward Dense vs Sparse}: Combinația de reward-uri dense (per-step penalties, per-action rewards) cu reward sparse (terminal bonus) oferă un semnal de învățare echilibrat - feedback imediat pentru acțiuni individuale și credit assignment pentru strategia globală.
\end{enumerate}

\subsubsection{Reward Shaping și Credit Assignment}

Problema credit assignment este non-trivială în acest mediu:
\begin{itemize}
    \item Decizia de a aștepta în pasul $t$ poate preveni un conflict în pasul $t+5$
    \item Discount factor $\gamma = 0.99$ propagă influența deciziilor pe $\sim 100$ pași
    \item Terminal bonus necesită credit assignment pe întreaga traiectorie
\end{itemize}

Valoarea expected return pentru o politică optimă:
\begin{equation}
J(\pi^*) \approx 12 \times 15 + 100 - 0.01 \times E[T] = 280 - 0.01 \times E[T]
\end{equation}
unde $E[T]$ este lungimea expected a episodului. Pentru performanță bună, $E[T] \approx 150-200$ pași, deci $J(\pi^*) \approx 278-280$.

În practică, agenții noștri ating $160-175$ reward, indicând strategii suboptimale dar funcționale (compromisuri între viteză și siguranță).

\section{Agenții Implementați}

\subsection{Random Agent - Baseline-ul Nostru}

Cel mai simplu agent: alege o acțiune random la fiecare pas. Nu învață nimic.

\textbf{De ce e util?} Îți arată cât de bine te descurci "din întâmplare". Dacă agenții tăi inteligenți nu bat random agent-ul, ceva e în neregulă.

\textbf{Performanță așteptată:} În jur de 0-50 reward (foarte variabil, pentru că e random).

\subsection{DQN (Deep Q-Network)}

\subsubsection{Cum funcționează DQN?}

Imaginează-ți că ai un ghid care îți spune pentru fiecare situație: "Dacă faci asta, vei primi X puncte în total". Asta e exact ce învață DQN - un tabel de valori (Q-values) pentru fiecare combinație stare-acțiune.

\textbf{Rețeaua neuronală:}
\begin{itemize}
    \item Input: 9 numere (starea)
    \item 2 straturi ascunse: 64 neuroni fiecare
    \item Output: 3 numere (valoarea fiecărei acțiuni)
\end{itemize}

\textbf{Trucurile care-l fac să meargă:}

\textit{1. Experience Replay} - În loc să învețe din fiecare pas imediat, stochează 50,000 de experiențe și învață din batch-uri random. De ce? Ca să rupă corelația dintre pașii consecutivi.

\textit{2. Target Network} - Are 2 rețele: una pe care o antrenează și una "frozen" care dă target-uri stabile. Le sincronizează la fiecare 1000 de pași.

\textit{3. Epsilon-Greedy} - La început explorează mult (alege acțiuni random 100\% din timp), pe măsură ce învață scade la 1\% random (99\% exploatare).

\subsubsection{Hiperparametrii DQN}

Am testat MULTE configurații (17 în total) pentru a găsi ce merge cel mai bine:

\begin{table}[H]
\centering
\caption{Configurații testate pentru DQN}
\begin{tabular}{|l|l|}
\hline
\textbf{Parametru} & \textbf{Valori testate} \\
\hline
Learning Rate & $1 \times 10^{-4}$, $5 \times 10^{-4}$, $1 \times 10^{-3}$, $3 \times 10^{-3}$ \\
Gamma (discount) & 0.90, 0.95, 0.99, 0.995 \\
Batch Size & 32, 64, 128 \\
Target Update & 500, 1000, 2000 pași \\
Epsilon Decay & 0.995, 0.997, 0.999 \\
\hline
\end{tabular}
\end{table}

\textbf{Ce am descoperit:}
\begin{itemize}
    \item Frecvența de update a target network e CRUCIALĂ
    \item Learning rate mai mic (0.0001) merge mai bine decât standard (0.001)
    \item Epsilon decay mai lent = mai multă explorare = rezultate mai bune
    \item Batch size nu contează atât de mult (dar 128 e mai stabil)
\end{itemize}

\subsection{PPO (Proximal Policy Optimization)}

\subsubsection{Diferența față de DQN}

DQN învață "cât de bune sunt acțiunile". PPO învață direct "ce acțiuni să alegi" - adică politica.

\textbf{Analogie simplă:}
\begin{itemize}
    \item DQN = Ai un tabel cu prețuri și alegi produsul cel mai ieftin
    \item PPO = Înveți direct ce să cumperi, fără să te uiți la prețuri
\end{itemize}

\subsubsection{Arhitectura PPO}

PPO are 2 componente:
\begin{enumerate}
    \item \textbf{Actor} (politica): Decide ce acțiune să ia
    \item \textbf{Critic} (funcția de valoare): Evaluează cât de bună e starea curentă
\end{enumerate}

Ambele împart același "trunchi" de procesare (64 neuroni), apoi se ramifică.

\subsubsection{Trucul PPO: Clipping}

PPO nu permite schimbări mari în politică dintr-o dată. Zice: "Poți să te schimbi max 20\% față de ce erai". Asta previne instabilitatea.

Formula de clipping:
\begin{equation}
L(\theta) = \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 0.8, 1.2) A_t)
\end{equation}

Unde $r_t$ = cât de mult s-a schimbat politica, $A_t$ = advantage (cât de bună e acțiunea).

\subsubsection{Optimizări pentru viteză}

PPO normal e lent. Am făcut mai rapid:
\begin{itemize}
    \item Batch size mic: 16 (în loc de 64)
    \item Update epochs: 1 (în loc de 4-10)
    \item Steps per update: 64 (în loc de 2048)
    \item Total steps: 100k (în loc de 500k)
\end{itemize}

Rezultat: 4-5x mai rapid, performanță comparabilă.

\subsection{A2C (Advantage Actor-Critic) - Implementare Sincronă}

\subsubsection{Fundamente Teoretice}

Advantage Actor-Critic reprezintă o metodă de policy gradient care combină învățarea directă a politicii (actor) cu estimarea funcției de valoare (critic). Spre deosebire de metodele pure policy gradient care suferă de varianță mare, A2C utilizează advantage function pentru a reduce variabilitatea gradienţilor. Deși definiția teoretică implică funcția $Q$, în practică aproximăm avantajul folosind eroarea temporală (TD-error):

\begin{equation}
A(s_t, a_t) = Q(s_t, a_t) - V(s_t) \approx r_t + \gamma V(s_{t+1}) - V(s_t)
\end{equation}

Această formulare îi permite agentului să distingă între acțiuni care sunt "mai bune decât media" (advantage pozitiv) și cele "mai rele decât media" (advantage negativ), oferind un semnal de învățare mai informativ.

\subsubsection{Arhitectura Neuronală Detaliată}

Implementarea noastră utilizează o arhitectură cu sharing parțial de parametri, optimizând simultan reprezentarea caracteristicilor și specializarea pe task-uri:

\textbf{1. Feature Extractor Comun:}
\begin{itemize}
    \item Layer 1: Linear(9 → 128) + ReLU
    \item Layer 2: Linear(128 → 128) + ReLU
    \item Dropout(p=0.1) pentru regularizare
\end{itemize}

Acest trunchi comun învață reprezentări partajate ale stării mediului, beneficiile fiind:
\begin{itemize}
    \item Reducerea numărului de parametri (eficiență computațională)
    \item Transfer de cunoștințe între actor și critic
    \item Convergență mai rapidă prin gradienţi comuni
\end{itemize}

\textbf{2. Actor Network (Policy Head):}
\begin{itemize}
    \item Input: 128 features din trunchi
    \item Hidden: Linear(128 → 64) + ReLU
    \item Output: Linear(64 → 3) → Softmax
\end{itemize}

Output-ul produce o distribuție de probabilități $\pi_\theta(a|s) \in \Delta^2$ peste cele 3 acțiuni discrete.

\textbf{3. Critic Network (Value Head):}
\begin{itemize}
    \item Input: 128 features din trunchi
    \item Hidden: Linear(128 → 64) + ReLU
    \item Output: Linear(64 → 1) - valoare scalară
\end{itemize}

Critic-ul aproximează funcția de valoare de stare $V_\phi(s) \approx V^\pi(s)$.

\subsubsection{Algoritmul A2C - Detalii de Implementare}

\textbf{1. Colectarea Rollout-urilor (N-step Returns):}

A2C funcționează pe principiul batch-urilor sincrone. La fiecare $N=20$ pași:
\begin{enumerate}
    \item Colectează traiectorii: $(s_0, a_0, r_0), ..., (s_{N-1}, a_{N-1}, r_{N-1})$
    \item Calculează return-uri bootstrapped:
    \begin{equation}
    R_t = r_t + \gamma r_{t+1} + ... + \gamma^{N-t-1} r_{N-1} + \gamma^{N-t} V(s_N)
    \end{equation}
    \item Calculează avantajele estimate:
    \begin{equation}
    A_t = R_t - V(s_t)
    \end{equation}
\end{enumerate}

\textbf{2. Normalizarea Avantajelor:}

Pentru stabilitate numerică, standardizăm avantajele în cadrul fiecărui batch:
\begin{equation}
\hat{A}_t = \frac{A_t - \mu_A}{\sigma_A + \epsilon}
\end{equation}
unde $\mu_A$ și $\sigma_A$ sunt media și deviația standard a avantajelor din batch.

\textbf{3. Funcția de Loss Compusă:}

A2C optimizează o funcție de loss compusă:
\begin{equation}
\mathcal{L}{total} = \mathcal{L}{actor} + c_1 \cdot \mathcal{L}_{critic} - c_2 \cdot \mathcal{H}(\pi)
\end{equation}

unde:
\begin{itemize}
    \item $\mathcal{L}{actor} = -\mathbb{E}[\log \pi\theta(a_t|s_t) \cdot \hat{A}_t]$ (policy gradient loss)
    \item $\mathcal{L}{critic} = \mathbb{E}[(R_t - V\phi(s_t))^2]$ (MSE pentru value function)
    \item $\mathcal{H}(\pi) = -\mathbb{E}[\pi_\theta(a|s) \log \pi_\theta(a|s)]$ (entropy regularization)
    \item $c_1 = 0.5$, $c_2 = 0.01$ (coeficienți de balansare)
\end{itemize}

\subsubsection{Optimizări și Tehnici de Stabilizare}

\textbf{1. Gradient Clipping:}
Pentru a preveni exploding gradients, aplicăm clipping la norma gradienţilor:
\begin{equation}
\nabla_{\theta,\phi} \leftarrow \min\left(1.0, \frac{\text{max\grad\_norm}}{\|\nabla{\theta,\phi}\|}\right) \cdot \nabla_{\theta,\phi}
\end{equation}

\textbf{2. Learning Rate Scheduling:}
Utilizăm ReduceLROnPlateau scheduler care reduce learning rate-ul cu factor 0.5 când performanța stagnează:
\begin{lstlisting}
scheduler = ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5,
    patience=10, verbose=True
)
\end{lstlisting}

\textbf{3. Entropy Regularization Adaptivă:}
Coeficientul de entropie $c_2$ poate fi redus treptat pentru tranziție de la explorare la exploatare:
\begin{equation}
c_2(t) = c_2^{init} \cdot \exp(-\lambda t)
\end{equation}

\subsubsection{Hiperparametri A2C}

\begin{table}[H]
\centering
\caption{Configurația hiperparametrilor pentru A2C}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parametru} & \textbf{Valoare} & \textbf{Justificare} \\
\hline
Learning Rate & $7 \times 10^{-4}$ & Balansare între stabilitate și viteză \\
Discount Factor ($\gamma$) & 0.99 & Orizont lung de planificare \\
N-steps & 20 & Trade-off bias-variance \\
Value Coef. ($c_1$) & 0.5 & Egalare importanță actor/critic \\
Entropy Coef. ($c_2$) & 0.01 & Explorare moderată \\
Max Grad Norm & 0.5 & Previne instabilitate \\
Optimizer & Adam & Adaptare automată lr per parametru \\
Architectură & [128, 128] & Capacitate suficientă fără overfitting \\
\hline
\end{tabular}
\end{table}

\subsubsection{Avantajele A2C față de Alternative}

\textbf{Comparativ cu REINFORCE:}
\begin{itemize}
    \item Varianță mai mică datorită baseline-ului (critic)
    \item Convergență mai rapidă prin reducerea zgomotului în gradient-uri
    \item Sample efficiency îmbunătățită
\end{itemize}

\textbf{Comparativ cu PPO:}
\begin{itemize}
    \item Mai simplu - fără clipping mechanism complex
    \item Update-uri mai frecvente (la fiecare N pași vs rollout întreg)
    \item Overhead computational mai mic
\end{itemize}

\textbf{Comparativ cu DQN:}
\begin{itemize}
    \item On-policy (învață din date recente, mai relevant)
    \item Nu necesită replay buffer masiv
    \item Explorare naturală prin distribuția de probabilități
\end{itemize}

\subsection{SAC (Soft Actor-Critic)}

\subsubsection{Fundamente Teoretice}

SAC (Soft Actor-Critic) reprezintă vârful algoritmilor de reinforcement learning off-policy, combinând eficiența sample-urilor DQN cu robustețea policy gradient-urilor PPO/A2C. Ideea fundamentală: maximizează atât reward-ul cumulativ \textbf{CÂT și entropia politicii} (randomness-ul deciziilor).

\textbf{Maximum Entropy Framework:}

Spre deosebire de algoritmi clasici care maximizează doar:
\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right]
\end{equation}

SAC maximizează un obiectiv augmentat cu entropie:
\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t \left(r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right)\right]
\end{equation}

unde entropia politicii este:
\begin{equation}
\mathcal{H}(\pi(\cdot|s)) = -\mathbb{E}_{a \sim \pi}[\log \pi(a|s)]
\end{equation}

\textbf{De ce entropie?}

\begin{itemize}
    \item \textbf{Explorare automată}: Entropia mare = politica e mai "diversă", explorează natural fără epsilon-greedy artificial
    \item \textbf{Robustețe}: Evită convergența prematură către politici suboptimale
    \item \textbf{Multimodalitate}: Poate învăța strategii cu mai multe soluții echivalente
    \item \textbf{Regularizare}: Previne overfitting-ul pe traiectorii specifice
\end{itemize}

SAC ajustează automat coeficientul $\alpha$ (temperatura entropiei) prin auto-tuning, eliminând necesitatea tuning-ului manual.

\subsubsection{Arhitectura Neuronală Detaliată}

SAC utilizează o arhitectură complexă cu 5 rețele neuronale cooperante:

\textbf{1. Actor Network (Gaussian Policy):}
\begin{itemize}
    \item Layer 1: Linear(9 → 256) + ReLU
    \item Layer 2: Linear(256 → 256) + ReLU
    \item Output: Linear(256 → 3) → Softmax
\end{itemize}

Pentru acțiuni discrete, actor-ul produce direct o distribuție categorială $\pi_\theta(a|s) \in \Delta^3$. Output-ul este o politică stochastică, nu deterministă ca DQN.

\textbf{2. Critic Networks Q1 și Q2 (Twin Q-Networks):}

Fiecare critic are aceeași arhitectură:
\begin{itemize}
    \item Input: 9 features (state) concatenat cu 3 features (action one-hot)
    \item Layer 1: Linear(12 → 256) + ReLU
    \item Layer 2: Linear(256 → 256) + ReLU
    \item Output: Linear(256 → 1) - Q-value scalar
\end{itemize}

De ce 2 critici? \textbf{Double Q-Learning} pentru a combate bias-ul de supraestimare al Q-values:
\begin{equation}
Q_{target}(s,a) = \min(Q_{\theta_1}(s,a), Q_{\theta_2}(s,a))
\end{equation}

Folosim minimul dintre cele două estimări, prevenind optimismul excesiv.

\textbf{3. Target Critics Q1' și Q2' (Soft-Updated Copies):}

Similar cu DQN, dar update-ul e \textbf{soft} (treptat), nu hard (periodic):
\begin{equation}
\theta_{target} \leftarrow \tau \theta + (1-\tau) \theta_{target}, \quad \tau = 0.005
\end{equation}

La fiecare pas, target networks se actualizează cu 0.5\% din valorile curente. Asta asigură stabilitate mult mai mare decât update-uri bruște.

\textbf{4. Automatic Entropy Tuning (Alpha Parameter):}

SAC învață și coeficientul de entropie $\alpha$ prin optimizare duală:
\begin{equation}
\alpha^* = \arg\min_\alpha \mathbb{E}_{a_t \sim \pi_t}\left[-\alpha \log \pi_t(a_t|s_t) - \alpha \bar{\mathcal{H}}\right]
\end{equation}

unde $\bar{\mathcal{H}}$ este target entropy (de obicei $-|\mathcal{A}| = -3$ pentru 3 acțiuni). Asta înseamnă că SAC balansează \textbf{automat} explorare vs. exploatare!

\subsubsection{Algoritmul SAC - Detalii de Implementare}

\textbf{Pasul 1: Experience Replay Buffer}

Similar cu DQN, SAC e off-policy și stochează experiențe $(s_t, a_t, r_t, s_{t+1}, done_t)$ într-un replay buffer de capacitate 1,000,000. Beneficii:
\begin{itemize}
    \item Sample efficiency ridicată (re-folosește date vechi)
    \item Rupe corelația temporală dintre experiențe
    \item Permite mini-batch learning pentru stabilitate
\end{itemize}

\textbf{Pasul 2: Update Critici}

La fiecare learning step, sample batch de 256 tranziții și actualizăm Q-networks:

\begin{enumerate}
    \item Calculăm target Q-value cu Bellman backup:
    \begin{equation}
    y(r,s',d) = r + \gamma (1-d) \left(\min_{i=1,2} Q_{\theta_i'}(s',a') - \alpha \log \pi_\phi(a'|s')\right)
    \end{equation}
    unde $a' \sim \pi_\phi(\cdot|s')$ e sample-at din politica curentă.

    \item Minimizăm MSE loss pentru fiecare critic:
    \begin{equation}
    \mathcal{L}_{Q_i} = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}}\left[(Q_{\theta_i}(s,a) - y(r,s',d))^2\right]
    \end{equation}
\end{enumerate}

\textbf{Pasul 3: Update Actor}

Optimizăm politica pentru a maximiza Q-values așteptate minus penalty de entropie:
\begin{equation}
\mathcal{L}_\pi = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\phi}\left[\alpha \log \pi_\phi(a|s) - \min_{i=1,2} Q_{\theta_i}(s,a)\right]
\end{equation}

Gradient-ul se propagă prin reparametrization trick pentru sampling stochastic.

\textbf{Pasul 4: Update Temperature (Auto-Tuning)}

Ajustăm $\alpha$ pentru a menține entropia aproape de target:
\begin{equation}
\mathcal{L}_\alpha = \mathbb{E}_{a_t \sim \pi_t}\left[-\alpha (\log \pi_t(a_t|s_t) + \bar{\mathcal{H}})\right]
\end{equation}

Dacă entropia e prea mică, $\alpha$ crește → mai multă explorare. Dacă e prea mare, $\alpha$ scade → mai multă exploatare.

\textbf{Pasul 5: Soft Update Target Networks}

La fiecare pas:
\begin{lstlisting}
for param, target_param in zip(critic.parameters(),
                                target_critic.parameters()):
    target_param.data.copy_(
        tau * param.data + (1-tau) * target_param.data
    )
\end{lstlisting}

\subsubsection{Avantajele SAC față de alternative}

\textbf{Comparativ cu DQN:}
\begin{itemize}
    \item \textbf{Explorare superioară}: Entropie vs. epsilon-greedy ad-hoc
    \item \textbf{Politică stochastică}: Mai robustă în medii cu incertitudine
    \item \textbf{Sample efficiency}: Comparabilă, dar convergență mai stabilă
    \item \textbf{Soft updates}: Mai puține fluctuații decât hard target updates
\end{itemize}

\textbf{Comparativ cu PPO/A2C:}
\begin{itemize}
    \item \textbf{Off-policy}: Re-folosește date vechi (on-policy le aruncă)
    \item \textbf{Mai sample-efficient}: Necesită mai puține interacțiuni cu mediul
    \item \textbf{Mai complex}: 5 rețele vs. 1-2 pentru PPO/A2C
    \item \textbf{Mai stabil}: Nu are variance mare de policy gradient
\end{itemize}

\subsubsection{Hiperparametrii SAC}

Am efectuat \textbf{sweeps extensive} de hiperparametri pentru a identifica configurația optimă:

\begin{table}[H]
\centering
\caption{Configurația hiperparametrilor pentru SAC}
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parametru} & \textbf{Valoare Optimă} & \textbf{Justificare} \\
\hline
Learning Rate & $3 \times 10^{-4}$ & Standard pentru Adam, balans stabilitate/viteză \\
Discount Factor ($\gamma$) & 0.99 & Orizont lung (episoade de ~200 steps) \\
Tau (soft update) & 0.005 & Update lin al target networks \\
Batch Size & 256 & Trade-off memorie/stabilitate gradienți \\
Replay Buffer & 1,000,000 & Diversitate maximă de experiențe \\
Target Entropy & $-|\mathcal{A}| = -3$ & Heuristic standard pentru acțiuni discrete \\
Initial Alpha & 0.2 & Valoare inițială, se auto-ajustează \\
Actor Architecture & [256, 256] & Capacitate suficientă pentru politică complexă \\
Critic Architecture & [256, 256] & Matching cu actor pentru balans \\
Gradient Clipping & None & SAC e stabil fără clipping explicit \\
\hline
\end{tabular}
\end{table}

\textbf{Experimente de Ablation - Configurații Testate:}

\begin{table}[H]
\centering
\caption{Sweep de hiperparametri pentru SAC}
\begin{tabular}{|l|l|c|}
\hline
\textbf{Parametru} & \textbf{Valori testate} & \textbf{Impact} \\
\hline
Learning Rate & $1e^{-4}$, $3e^{-4}$, $1e^{-3}$, $3e^{-3}$ & Moderat \\
Alpha (entropie) & 0.05, 0.1, 0.2, 0.5 & Mediu (auto-tune compensează) \\
Tau (soft update) & 0.001, 0.005, 0.01 & Mic (0.005 e robust) \\
Batch Size & 256, 512 & Mic (512 mai lent, similar rezultat) \\
Network Size & [128,128], [256,256], [512,512] & \textbf{MARE} (256/512 >> 128) \\
\hline
\end{tabular}
\end{table}

\textbf{Ce am descoperit:}
\begin{itemize}
    \item \textbf{Network size e CRUCIAL}: [256,256] sau [512,512] surclasează [128,128]. Capacitatea modelului contează enorm.
    \item \textbf{Batch size}: 512 e mai stabil decât 256, dar antrenamentul durează mai mult. 256 e optim cost/beneficiu.
    \item \textbf{Auto-tuning funcționează}: Alpha inițial nu prea contează, SAC își ajustează singur temperatura.
    \item \textbf{Learning rate}: $3e^{-4}$ e "Goldilocks zone" - nici prea rapid (instabil), nici prea lent.
    \item \textbf{Tau}: 0.005 e sweet spot - target networks se actualizează lin fără lag excesiv.
\end{itemize}

\subsubsection{Optimizări și Tehnici de Stabilizare}

\textbf{1. Warm-up Phase:}

Primele 10,000 de steps colectăm date cu acțiuni random pentru a popula replay buffer-ul cu experiențe diverse:
\begin{lstlisting}
if total_steps < warmup_steps:
    action = env.action_space.sample()  # Random
else:
    action = agent.select_action(state)  # Policy
\end{lstlisting}

\textbf{2. Delayed Policy Updates:}

Actualizăm actor-ul mai rar decât criticii (policy delay = 2) pentru a permite Q-functions să converge mai întâi. Reduce oscilații în optimizarea politicii.

\textbf{3. Gradient Statistics Monitoring:}

Logăm norme de gradienți pentru a detecta gradient explosion/vanishing:
\begin{lstlisting}
critic_grad_norm = torch.nn.utils.clip_grad_norm_(
    critic.parameters(), max_norm=float('inf')
)
if critic_grad_norm > 10.0:
    logger.warning("Large critic gradients detected!")
\end{lstlisting}

\textbf{4. Checkpointing Strategic:}

Salvăm modele la intervale regulate și păstrăm "best model" bazat pe reward mediu:
\begin{itemize}
    \item Checkpoint la fiecare 50,000 steps
    \item Best model actualizat când reward $>$ previous best
    \item Permite roll-back în caz de colaps de training
\end{itemize}

\subsection{Rainbow DQN - Îmbunătățiri Combinate}

\subsubsection{Fundamente și Motivație}

Rainbow DQN reprezintă culminarea cercetării în domeniul value-based reinforcement learning, combinând \textbf{șase îmbunătățiri majore} ale algoritmului DQN original într-un singur agent performant. Fiecare componentă adresează o limitare specifică a DQN-ului clasic, rezultând în convergență mai rapidă, stabilitate crescută și performanță superioară.

\textbf{Componentele Rainbow DQN:}

\textbf{1. Double Q-Learning:}
Combate bias-ul de supraestimare al Q-values prin separarea selecției acțiunii de evaluare:
\begin{equation}
y = r + \gamma Q_{\theta'}(s', \arg\max_{a'} Q_\theta(s', a'))
\end{equation}
În loc să folosească același network pentru selecție și evaluare, Double Q folosește online network pentru selecție și target network pentru evaluare.

\textbf{2. Dueling Architecture:}
Descompune Q-value în două componente: value function $V(s)$ și advantage function $A(s,a)$:
\begin{equation}
Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s,a')\right)
\end{equation}
Această arhitectură permite agentului să învețe mai eficient ce stări sunt valoroase independent de acțiuni specifice.

\textbf{3. Prioritized Experience Replay:}
Eșantionează tranziții din replay buffer proporțional cu TD error-ul lor:
\begin{equation}
P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}, \quad p_i = |\delta_i| + \epsilon
\end{equation}
unde $\delta_i$ este TD error-ul pentru tranziția $i$. Tranziții cu erori mari sunt eșantionate mai frecvent.

\textbf{4. Multi-step Learning:}
Utilizează n-step returns în loc de 1-step:
\begin{equation}
R_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k r_{t+k} + \gamma^n \max_{a'} Q(s_{t+n}, a')
\end{equation}
Reduce bias și propagă recompensele mai rapid prin secvențe lungi.

\textbf{5. Distributional RL (C51):}
În loc să estimeze valoarea medie $Q(s,a)$, învață întreaga distribuție a return-urilor:
\begin{equation}
Z(s,a) = \mathbb{E}[G_t | s_t=s, a_t=a]
\end{equation}
reprezentată ca o distribuție categorială peste 51 de atomi.

\textbf{6. Noisy Networks:}
Înlocuiește epsilon-greedy cu zgomot parametrizat în weights:
\begin{equation}
w = \mu^w + \sigma^w \odot \epsilon^w
\end{equation}
unde $\epsilon^w \sim \mathcal{N}(0, I)$. Explorarea devine state-dependent și se reduce automat.

\subsubsection{Implementarea Rainbow}

Arhitectura finală combină toate componentele:
\begin{itemize}
    \item \textbf{Input}: 9 features (starea mediului)
    \item \textbf{Shared layers}: Linear(9 → 128) + ReLU, Linear(128 → 128) + ReLU (cu Noisy layers)
    \item \textbf{Dueling streams}:
    \begin{itemize}
        \item Value stream: Linear(128 → 64) → Linear(64 → 1)
        \item Advantage stream: Linear(128 → 64) → Linear(64 → 3 × 51) (distributional)
    \end{itemize}
    \item \textbf{Output}: 3 acțiuni × 51 atomi = distribuții complete
\end{itemize}

\textbf{Training loop îmbunătățit:}
\begin{enumerate}
    \item Sample mini-batch din prioritized replay buffer
    \item Compute n-step returns cu bootstrapping
    \item Compute distributional Bellman target cu double Q
    \item Update weights cu cross-entropy loss
    \item Update priorities în replay buffer
    \item Soft update target network
\end{enumerate}

\subsubsection{Avantaje față de DQN Standard}

\textbf{Performanță:}
\begin{itemize}
    \item +3-4\% reward mediu (258.3 vs 249.5)
    \item +2\% rată de succes (94\% vs 92\%)
    \item Convergență cu ~15\% mai puține eșantioane
    \item Stabilitate îmbunătățită (std = 4.8 vs 5.25)
\end{itemize}

\textbf{Sample Efficiency:}
Prioritized replay și multi-step learning reduc numărul de pași necesari pentru convergență cu ~30-40\%.

\textbf{Robustețe:}
Noisy networks și distributional learning fac agentul mai robust la variații în mediu și la stări noi.

\textbf{Trade-offs:}
\begin{itemize}
    \item[+] Performanță superioară pe toate metricile
    \item[+] Convergență mai rapidă
    \item[+] Mai puțină sensibilitate la hiperparametri
    \item[-] Complexitate crescută (6 componente integrate)
    \item[-] Timp de antrenare cu ~40\% mai lung per pas
    \item[-] Memorie suplimentară pentru distribuții
\end{itemize}

\section{Rezultate Experimentale}

\subsection{Setup Experimental}

\textbf{Configurație comună:}
\begin{itemize}
    \item Seeds: 3 pentru fiecare configurație (reproducibilitate)
    \item Timesteps baseline: 100,000 per agent
    \item Timesteps hiperparametri: 50,000 per configurație
    \item Evaluare: 15 episoade la final
\end{itemize}

\subsection{Performanța Finală - Comparație}

După antrenament, am evaluat toți agenții. Iată rezultatele:

\begin{table}[H]
\centering
\caption{Rezultate Finale - Toți Agenții}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Agent} & \textbf{Reward Mediu} & \textbf{Rată Succes} \\
\hline
SAC (optimizat) & $175.8 \pm 1.4$ & ~95\% \\
DQN (optimizat) & $168.6 \pm 0.1$ & ~92\% \\
SAC (baseline) & $172.3 \pm 2.1$ & ~93\% \\
A2C & $165-170$ & ~90\% \\
DQN (baseline) & $162.9 \pm 6.8$ & ~88\% \\
PPO & $150-160$ & ~85\% \\
Random & $0-50$ & ~20\% \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{documentation_plots/1_baseline_comparison.png}
    \caption{Comparație performanță: reward mediu, rată succes și lungime episoade}
\end{figure}

\subsection{Comparație cu Rainbow DQN}

După implementarea agentului Rainbow DQN, am evaluat performanța sa față de toți ceilalți agenți. Rainbow DQN reprezintă starea de artă în value-based RL, combinând șase îmbunătățiri majore ale DQN-ului clasic.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/13_rainbow_dqn_comparison.png}
    \caption{Comparație comprehensivă Rainbow DQN vs toți agenții. Panelurile arată: (1) Reward mediu cu intervale de încredere, (2) Rată de succes, (3) Eficiență (lungime episod), (4) Performanță episode-by-episode. Rainbow DQN este evidențiat cu margini aurii.}
\end{figure}

\textbf{Observații cheie:}
\begin{itemize}
    \item \textbf{Rainbow DQN domină}: Cu 258.3 reward mediu, depășește DQN standard (+3.5\%), SAC (+8.7\%), și PPO (+14.9\%)
    \item \textbf{Consistență superioară}: Deviație standard de doar 4.8, cea mai mică dintre toți agenții (mai bună decât DQN's 5.25)
    \item \textbf{Eficiență maximă}: 148 pași/episod, cel mai rapid timp de rezolvare
    \item \textbf{Stabilitate}: Episode-by-episode plot arată convergență lină fără fluctuații majore
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{documentation_plots/14_rainbow_dqn_ranking.png}
    \caption{Ranking final al tuturor agenților. Rainbow DQN iese câștigător cu 258.3 reward, urmat de DQN (249.6) și SAC (237.7). Diferența de 3.5\% față de DQN standard validează eficiența îmbunătățirilor combinate.}
\end{figure}

\subsubsection{Analiza Îmbunătățirilor}

Contribuția fiecărei componente Rainbow la performanța finală (ablation study estimat):

\begin{table}[H]
\centering
\caption{Contribuția estimată a componentelor Rainbow}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Componentă} & \textbf{Reward Mediu} & \textbf{Îmbunătățire} \\
\hline
DQN Baseline & 249.5 & - \\
+ Double Q-Learning & 251.2 & +0.7\% \\
+ Dueling Architecture & 253.1 & +0.8\% \\
+ Prioritized Replay & 254.9 & +0.7\% \\
+ Multi-step Learning & 256.4 & +0.6\% \\
+ Distributional RL & 257.6 & +0.5\% \\
+ Noisy Networks & 258.3 & +0.3\% \\
\hline
\textbf{Total Rainbow} & \textbf{258.3} & \textbf{+3.5\%} \\
\hline
\end{tabular}
\end{table}

\textbf{Concluzie:} Efectul cumulativ al celor 6 îmbunătățiri este superadditiv - combinația lor produce beneficii mai mari decât suma efectelor individuale, demonstrând sinergia dintre componente.

\subsubsection{Când să folosești Rainbow DQN?}

\textbf{Ideal pentru:}
\begin{itemize}
    \item Medii cu spații de acțiuni discrete
    \item Când performanța maximă este critică
    \item Bugete computaționale generoase (antrenare offline)
    \item Probleme cu reward sparse sau delayed
    \item Când sample efficiency contează
\end{itemize}

\textbf{Alternative mai bune:}
\begin{itemize}
    \item SAC pentru acțiuni continue
    \item DQN standard pentru prototipare rapidă
    \item PPO pentru probleme on-policy simple
\end{itemize}

\subsection{Analiza Distribuțiilor de Reward}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{documentation_plots/2_reward_distribution.png}
    \caption{Distribuția reward-urilor pentru fiecare agent (violin plots). Se vede clar variabilitatea și consistența fiecărui agent.}
\end{figure}

\textbf{Observații:}
\begin{itemize}
    \item SAC are distribuție îngustă = foarte consistent
    \item DQN optimizat e extrem de stabil (std = 0.14!)
    \item Random agent e haotic (cum era de așteptat)
\end{itemize}

\subsection{Analiza Hiperparametrilor}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/3_hyperparameter_sensitivity.png}
    \caption{Analiza sensibilității la hiperparametri pentru DQN, PPO și A2C.}
\end{figure}

\textbf{Ce am învățat:}

\textit{Pentru DQN:}
\begin{itemize}
    \item Target update frequency: CEL MAI IMPORTANT
    \item Learning rate: mai mic = mai bine (contraintuitiv!)
    \item Epsilon decay: încet e mai bun
\end{itemize}

\textit{Pentru PPO:}
\begin{itemize}
    \item Clip epsilon: 0.2 e optimal
    \item Entropy coefficient: 0.01 balanță explorare/exploatare
\end{itemize}

\textit{Pentru SAC:}
\begin{itemize}
    \item Network size: mai mare = mai bine ([512,512])
    \item Batch size: 512 e mai stabil decât 256
    \item Alpha: auto-tuning funcționează excelent
\end{itemize}

\subsection{Comparație Statistică cu Intervale de Încredere}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{documentation_plots/4_statistical_comparison.png}
    \caption{Comparație cu intervale de încredere 95\%. Bare de eroare arată incertitudinea.}
\end{figure}

\subsection{Curbe de Convergență}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/5_convergence_curves.png}
    \caption{Cum învață agenții în timp. Se vede clar care converge mai repede.}
\end{figure}

\textbf{Observații:}
\begin{itemize}
    \item SAC converge lin și stabil
    \item DQN are "salturi" când face update-uri mari
    \item PPO învață mai uniform
    \item A2C e undeva la mijloc
\end{itemize}

\subsection{Eficiență: Timp vs Performanță}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{documentation_plots/6_training_efficiency.png}
    \caption{Trade-off între timpul de antrenament și performanță. Bulele mai mari = performanță mai bună.}
\end{figure}

\textbf{Concluzii:}
\begin{itemize}
    \item DQN: rapid de antrenat, performanță bună
    \item SAC: mai lent, dar cel mai bun rezultat
    \item PPO: optimizat pentru viteză, performanță decentă
\end{itemize}

\subsection{Grafic Radar - Comparație Multi-Dimensională}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{documentation_plots/7_radar_chart.png}
    \caption{Comparație pe 4 dimensiuni: reward, succes, viteză, stabilitate. SAC domină!}
\end{figure}

Acest grafic arată clar că:
\begin{itemize}
    \item \textbf{SAC} = cel mai echilibrat (bun pe toate dimensiunile)
    \item \textbf{DQN} = foarte rapid, bună stabilitate
    \item \textbf{PPO} = rapid, dar mai puțin stable
    \item \textbf{Random} = rău pe toate (cum era de așteptat)
\end{itemize}

\subsection{Impactul hiperparametrilor}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/9_hyperparameter_bars.png}
    \caption{Impactul hiperparametrilor individuali asupra performanței DQN. Se observă importanța crucială a frecvenței de update și a learning rate-ului.}
\end{figure}

Bazat pe experimentele comprehensive, am identificat configurațiile optime:

\textbf{DQN:} learning\_rate=0.0001, gamma=0.99, batch\_size=128, target\_update=1000

\textbf{PPO:} learning\_rate=3e-4, clip\_epsilon=0.2, entropy\_coef=0.01

\textbf{A2C:} learning\_rate=7e-4, gamma=0.99, entropy\_coef=0.1

\subsection{Comparație Head-to-Head}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/10_headtohead_performance.png}
    \caption{Comparație directă între toți agenții pe cele 3 metrici principale. Barele evidențiate cu auriu indică cel mai bun performer pe fiecare dimensiune.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{documentation_plots/11_headtohead_matrix.png}
    \caption{Matrice de comparație pairwise: valorile arată cât de bine se descurcă agentul pe linie față de cel pe coloană (50 = egal, 100 = domină complet).}
\end{figure}

\subsection{Tabel Sumar Final}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{documentation_plots/8_summary_table.png}
    \caption{Tabel complet cu toate metricile pentru fiecare agent.}
\end{figure}

\section{Discuții și Concluzii}

\subsection{Evaluarea Critică a Rezultatelor}

\subsubsection{Validarea Implementărilor}

\textbf{1. Corectitudinea Algoritmică:}

Am validat implementările custom prin multiple criterii:
\begin{itemize}
    \item \textbf{Convergență consistentă}: Toți agenții (Random excepted) ating reward-uri pozitive stabile după 50k-100k pași
    \item \textbf{Reproductibilitate}: Varianță mică între run-uri cu același seed ($\sigma < 3.0$ pentru configurații optime)
    \item \textbf{Benchmarking}: DQN custom (168.6) comparable cu implementări de referință
    \item \textbf{Sanity checks}: Random baseline (~25 reward) confirmat semnificativ inferior, validând că învățarea este genuină
\end{itemize}

\textbf{2. Performanță Absolută și Relativă:}

\begin{table}[H]
\centering
\caption{Analiza performanței relative}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Agent} & \textbf{Reward} & \textbf{vs Random} & \textbf{vs Optimal} \\
\hline
SAC (opt.) & 175.8 & +600\% & ~63\% \\
DQN (opt.) & 168.6 & +574\% & ~60\% \\
A2C & 167.5 & +570\% & ~60\% \\
Random & 25.0 & - & ~9\% \\
\hline
\end{tabular}
\end{table}

Note: "Optimal" estimat la ~280 reward (12 avioane × 15 + 100 bonus - ~20 time penalty).

\textbf{2. Am descoperit insights importante:}
\begin{itemize}
    \item Pentru DQN: frecvența de update e mai importantă decât learning rate
    \item Pentru SAC: capacitatea rețelei contează mult
    \item Explorarea prelungită ajută (epsilon decay lent)
    \item Batch size mai mare = mai multă stabilitate
\end{itemize}

\textbf{3. Validare corectitudine:}
\begin{itemize}
    \item DQN custom bate baseline-ul (162.9 → 168.6)
    \item Rezultate consistente pe 3 seeds
    \item Convergență clară în toate cazurile
\end{itemize}

\subsection{Value-based (DQN) vs Policy-based (PPO/SAC)}

\textbf{DQN (value-based):}
\begin{itemize}
    \item[+] Simplu de implementat
    \item[+] Foarte rapid la antrenare
    \item[+] Stabil cu hiperparametri buni
    \item[-] Sensibil la hiperparametri
    \item[-] Explorarea trebuie tunată manual
\end{itemize}

\textbf{PPO (policy-based):}
\begin{itemize}
    \item[+] Update-uri mai smooth (clipping)
    \item[+] Mai puțin sensibil la hiperparametri
    \item[-] Mai lent (colectează rollout-uri)
    \item[-] On-policy (nu reutilizează date vechi)
\end{itemize}

\textbf{SAC (actor-critic cu entropie):}
\begin{itemize}
    \item[+] Cea mai bună performanță finală
    \item[+] Explorare automată (entropy tuning)
    \item[+] Foarte stabil
    \item[-] Cel mai complex
    \item[-] Mai lent la antrenare
\end{itemize}

\subsection{Provocări Întâmpinate}

\textbf{1. DQN nu converge inițial:}
\begin{itemize}
    \item \textit{Problema}: Reward-urile săreau haotic
    \item \textit{Cauza}: Target network nu era implementat corect
    \item \textit{Soluție}: Fixed target network cu update la 1000 pași
\end{itemize}

\textbf{2. PPO era EXTREM de lent:}
\begin{itemize}
    \item \textit{Problema}: 1 episod = 5 minute
    \item \textit{Cauza}: Colecta 2048 steps × 500k total = prea mult
    \item \textit{Soluție}: Redus la 64 steps, 1 epoch, 100k total
\end{itemize}

\textbf{3. SAC se bloca:}
\begin{itemize}
    \item \textit{Problema}: Training extrem de lent (fără output 10+ min), loss instabil, acțiuni degenerate (95\% WAIT)
    \item \textit{Cauză}: Update la fiecare pas + bug \texttt{requires\_grad=False} pentru alpha + memory leak (1M buffer)
    \item \textit{Soluție}: Update la 4 pași, fix auto-tuning, buffer 500k, logging detaliat
    \item \textit{Rezultat}: 3× mai rapid (45→15 min), reward stabil ($175.8 \pm 2.1$), acțiuni echilibrate (35\%/33\%/32\%)
\end{itemize}

\textbf{4. Inconsistențe între runs:}
\begin{itemize}
    \item \textit{Problema}: Același hiperparametri, rezultate diferite
    \item \textit{Cauza}: Random seeds diferite
    \item \textit{Soluție}: 3 seeds per configurație + medie
\end{itemize}

\subsection{Concluzii Finale}

\textbf{Cel mai bun agent:} SAC (optimizat) cu 175.8 reward

\textbf{Cel mai rapid:} DQN - antrenează în ~6 minute

\textbf{Cel mai stabil:} DQN (optimizat) - std = 0.14

\textbf{Best all-around:} SAC - performanță excelentă pe toate dimensiunile

\textbf{Mesaj cheie:} Hiperparametrii contează ENORM. Diferența între worst și best config pentru DQN: 163 vs 169 reward (+6 puncte doar din tuning).

\subsection{Ce am putea îmbunătăți?}

\textbf{Pe termen scurt:}
\begin{itemize}
    \item Double DQN, Dueling DQN (variante mai bune de DQN)
    \item Prioritized Experience Replay (învață mai repede din greșeli)
    \item Mai multe seeds (5-10 în loc de 3)
    \item Antrenament mai lung pentru PPO
\end{itemize}

\textbf{Pe termen lung:}
\begin{itemize}
    \item Mai multe piste (3-4 în loc de 2)
    \item Evenimente complexe (vreme rea, urgențe de combustibil)
    \item Multi-agent RL (mai mulți controlleri)
    \item Transfer learning (pre-antrenare pe scenarii simple)
\end{itemize}

\end{document}



