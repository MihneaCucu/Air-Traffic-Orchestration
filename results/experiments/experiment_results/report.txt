================================================================================
RAPORT EXPERIMENTE - CUSTOM DQN
================================================================================

1. CONFIGURAȚII TESTATE
--------------------------------------------------------------------------------

Configurație: baseline
  - Learning Rate: 0.001
  - Gamma: 0.99
  - Batch Size: 64
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 162.92 ± 6.84

Configurație: freq_target_update
  - Learning Rate: 0.001
  - Gamma: 0.99
  - Batch Size: 64
  - Buffer Size: 50000
  - Target Update Frequency: 500
  - Rezultat: 168.57 ± 0.14

Configurație: high_gamma
  - Learning Rate: 0.001
  - Gamma: 0.995
  - Batch Size: 64
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 166.31 ± 2.75

Configurație: high_lr
  - Learning Rate: 0.005
  - Gamma: 0.99
  - Batch Size: 64
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 164.56 ± 3.17

Configurație: large_batch
  - Learning Rate: 0.001
  - Gamma: 0.99
  - Batch Size: 128
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 167.79 ± 1.10

Configurație: low_gamma
  - Learning Rate: 0.001
  - Gamma: 0.95
  - Batch Size: 64
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 163.19 ± 7.53

Configurație: low_lr
  - Learning Rate: 0.0001
  - Gamma: 0.99
  - Batch Size: 64
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 168.33 ± 0.44

Configurație: slow_epsilon_decay
  - Learning Rate: 0.001
  - Gamma: 0.99
  - Batch Size: 64
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 168.45 ± 0.59

Configurație: small_batch
  - Learning Rate: 0.001
  - Gamma: 0.99
  - Batch Size: 32
  - Buffer Size: 50000
  - Target Update Frequency: 1000
  - Rezultat: 164.75 ± 5.88


2. ANALIZA REZULTATELOR
--------------------------------------------------------------------------------

Cea mai bună configurație: freq_target_update
  - Reward mediu: 168.57 ± 0.14
  - Parametri: LR=0.001, Gamma=0.99, Batch=64

Cea mai slabă configurație: baseline
  - Reward mediu: 162.92 ± 6.84

3. CONCLUZII
--------------------------------------------------------------------------------

- Experimentele au fost realizate cu 3 seed-uri diferite pentru fiecare configurație
- Variabilele testate: learning rate, gamma, batch size, epsilon decay
- Diferența între cea mai bună și cea mai slabă: 5.65
- Stabilitatea antrenamentului (std dev) variază între configurații

================================================================================
