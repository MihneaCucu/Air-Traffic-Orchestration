# ‚úàÔ∏è Air Traffic Control - Reinforcement Learning Project

A sophisticated multi-agent reinforcement learning environment for simulating air traffic control operations with realistic physics, weather effects, and safety constraints.

## üéØ Project Overview

This project implements an **Air Traffic Control (ATC) simulation environment** where RL agents learn to manage aircraft takeoffs and landings across multiple runways while avoiding collisions, managing fuel constraints, and handling dynamic weather conditions.

**Team Members:** [Add your names here]

---

## üìã Problem Formulation

### State Space (9 dimensions)
- `planes_in_queue` - Number of aircraft waiting for departure
- `dep_occupied[0]` - Runway 1 departure status
- `dep_y[0]` - Runway 1 departure position
- `dep_occupied[1]` - Runway 2 departure status
- `dep_y[1]` - Runway 2 departure position
- `arrival_active` - Incoming aircraft status
- `arrival_lane` - Arrival runway assignment
- `arrival_y` - Arrival aircraft position
- `arrivals_landed` - Total successful landings

### Action Space (3 discrete actions)
- **0**: Wait/Hold - No action taken
- **1**: Clear Runway 1 - Authorize departure or landing
- **2**: Clear Runway 2 - Authorize departure or landing

### Reward Structure
| Event | Reward | Rationale |
|-------|--------|-----------|
| Time step | -0.01 | Encourage efficiency |
| Critical fuel | -0.2/plane/step | Prevent fuel emergencies |
| Departure cleared | +1.0 | Positive action feedback |
| Successful takeoff | +15.0 | Main objective |
| Successful landing | +15.0 | Main objective |
| Minor violation | -5.0 | Discourage unsafe operations |
| Critical violation | -10.0 | Strong penalty for danger |
| Episode completion | +100.0 | Bonus for clearing all aircraft |

### Environment Dynamics
- **Weather System**: Wind speed affects aircraft movement (0-60 km/h)
- **Fuel Management**: Each queued aircraft has fuel timer (critical < 15 steps)
- **Stochastic Arrivals**: 20% chance of incoming aircraft when clearing runway
- **Safety Zones**: Proximity violations occur when aircraft are < 2 units apart
- **Physics Simulation**: Realistic takeoff/landing with ground roll and climb phases

---

## üèóÔ∏è Project Structure

```
RL/
‚îú‚îÄ‚îÄ README.md                     # This file
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ train_dqn.py                 # Quick training script
‚îú‚îÄ‚îÄ visualize.py                 # Quick visualization script
‚îú‚îÄ‚îÄ run_tests.py                 # Environment testing
‚îÇ
‚îú‚îÄ‚îÄ src/                         # Source code
‚îÇ   ‚îú‚îÄ‚îÄ environment/             # Environment implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ atc_env.py          # Main ATC environment
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ agents/                  # RL Agent implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ custom_dqn_agent.py # Custom DQN implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model.py            # Neural network architectures
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/                # Training scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_custom_dqn.py # DQN training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py            # Multi-agent training
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_experiments.py  # Hyperparameter experiments
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/              # Evaluation & analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval.py             # Model evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualize_agent.py  # Agent visualization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compare_agents.py   # Multi-agent comparison
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyze_experiments.py # Results analysis
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ visualization/           # Plotting utilities
‚îÇ       ‚îú‚îÄ‚îÄ generate_all_plots.py
‚îÇ       ‚îú‚îÄ‚îÄ generate_hyperparameter_plots.py
‚îÇ       ‚îî‚îÄ‚îÄ generate_documentation_plots.py
‚îÇ
‚îú‚îÄ‚îÄ models/                      # Trained models
‚îÇ   ‚îî‚îÄ‚îÄ custom_dqn_atc.pth
‚îÇ
‚îú‚îÄ‚îÄ logs/                        # Training logs
‚îÇ   ‚îú‚îÄ‚îÄ tensorboard/            # TensorBoard logs
‚îÇ   ‚îî‚îÄ‚îÄ atc_logs/               # Custom logs
‚îÇ
‚îú‚îÄ‚îÄ results/                     # Experiment results
‚îÇ   ‚îú‚îÄ‚îÄ experiments/            # Raw experiment data
‚îÇ   ‚îî‚îÄ‚îÄ plots/                  # Generated visualizations
‚îÇ
‚îú‚îÄ‚îÄ tests/                       # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ test_enhanced_ui.py
‚îÇ
‚îî‚îÄ‚îÄ docs/                        # Documentation
    ‚îî‚îÄ‚îÄ documentation.tex
```

---

## üöÄ Installation & Setup

### Prerequisites
- Python 3.8+
- pip package manager

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Key Dependencies
- `gymnasium` - RL environment framework
- `pygame` - Visualization and rendering
- `numpy` - Numerical computations
- `torch` - Deep learning framework
- `stable-baselines3` - RL algorithms library
- `matplotlib` - Plotting
- `tensorboard` - Training visualization

---

## üéÆ Usage

### Quick Start - Train DQN Agent
```bash
python train_dqn.py
```

### Visualize Trained Agent
```bash
python visualize.py
```

### Run Environment Tests
```bash
python run_tests.py
```

### Advanced Training

#### Train Custom DQN
```bash
python src/training/train_custom_dqn.py
```

#### Train Multiple Agents (PPO & DQN)
```bash
python src/training/train.py
```

#### Run Hyperparameter Experiments
```bash
python src/training/run_experiments.py
```

### Evaluation & Analysis

#### Evaluate Agent Performance
```bash
python src/evaluation/eval.py
```

#### Compare Multiple Agents
```bash
python src/evaluation/compare_agents.py
```

#### Analyze Experiment Results
```bash
python src/evaluation/analyze_experiments.py
```

#### Generate Plots
```bash
python src/visualization/generate_all_plots.py
```

---

## ü§ñ Implemented Agents

### 1. **Custom DQN** (Deep Q-Network)
- **Type**: Value-based, off-policy
- **Architecture**: 3-layer MLP (64-64 hidden units)
- **Features**: Experience replay, target network, Œµ-greedy exploration
- **Hyperparameters**:
  - Learning rate: 1e-3
  - Gamma: 0.99
  - Batch size: 64
  - Buffer size: 50,000
  - Target update frequency: 1,000 steps

### 2. **[Agent 2 - To be implemented by teammate]**
- Type: [e.g., Policy-based, Actor-Critic]
- Details: [Add implementation details]

### 3. **[Agent 3 - To be implemented by teammate]**
- Type: [e.g., Model-based, Tabular]
- Details: [Add implementation details]

---

## üìä Experiments & Results

### Experiment Categories

1. **Baseline Performance**
   - Default hyperparameters
   - Multiple seeds for statistical significance

2. **Hyperparameter Tuning**
   - Learning rate: [1e-4, 1e-3, 1e-2]
   - Gamma (discount factor): [0.95, 0.99, 0.999]
   - Batch size: [32, 64, 128]
   - Target update frequency: [500, 1000, 2000]
   - Epsilon decay: [slow, medium, fast]

3. **Agent Comparison**
   - Same environment conditions
   - Equal training timesteps
   - Consistent evaluation protocol

### Performance Metrics
- **Average Reward**: Mean episodic return
- **Success Rate**: % episodes with all aircraft cleared
- **Safety Score**: Violations per episode
- **Convergence Speed**: Steps to reach threshold performance
- **Stability**: Reward variance across seeds

### Sample Results (Custom DQN)
```
Episode 1: 260.4
Episode 2: 297.4
Episode 3: 264.8
Episode 4: 278.4
Episode 5: 259.1

Average: 272.0 ¬± 15.8
```

---

## üé® Enhanced UI Features

### Visual Elements
- ‚úÖ **Dynamic gradient backgrounds** adapting to weather
- ‚úÖ **Animated clouds** with parallax effect
- ‚úÖ **Realistic rain effects** with wind influence
- ‚úÖ **3D altitude rendering** with dynamic shadows
- ‚úÖ **Exhaust trails** for departing aircraft
- ‚úÖ **Modern information panels** with live statistics

### Information Display
- **Left Panel**: Arrivals statistics with aircraft icons
- **Top Center**: Weather, safety score, violations
- **Bottom Panel**: Departure queue with fuel indicators
- **Real-time Feedback**: Action notifications and alerts
- **Proximity Warnings**: Visual alerts for near-misses

### Rendering Improvements
- 8 frames/step smooth animations
- 800x700 window resolution
- Professional color schemes
- Rounded UI elements
- Status-coded information (red/yellow/green)

---

## üìà Grading Criteria Alignment

| Category | Points | Status |
|----------|--------|--------|
| **Theme & Problem** (1p) | | |
| Clear RL-relevant theme | 0.5p | ‚úÖ |
| Well-defined state/action/reward | 0.5p | ‚úÖ |
| **Environment** (2p) | | |
| Functional & correct | 1p | ‚úÖ |
| Significant modifications/custom | 0.5p | ‚úÖ |
| Good reward design & dynamics | 0.5p | ‚úÖ |
| **Algorithms** (3p) | | |
| 3+ correct implementations | 2p | üîÑ 1/3 done |
| Algorithm diversity | 0.5p | üîÑ Pending |
| Fair comparison | 0.5p | üîÑ Pending |
| **Experiments** (2p) | | |
| Multiple seeds/experiments | 1p | ‚úÖ |
| Hyperparameter analysis | 0.5p | ‚úÖ |
| Stability/convergence discussion | 0.5p | üîÑ Pending |
| **Results** (2p) | | |
| Graphs/tables | 1p | üîÑ Pending |
| Interpretation | 1p | üîÑ Pending |
| **Documentation** (2p) | | |
| Structured documentation | 1p | üîÑ In progress |
| Coherent presentation | 1p | üîÑ Pending |
| **Bonus** (+1p) | | |
| Advanced features | +1p | üîÑ Candidate |

---

## üéØ Next Steps

### For Teammates
1. **Implement additional agents** in `src/agents/`:
   - Suggested: PPO (policy-based), SARSA (tabular), A3C (actor-critic)
   - Follow the structure of `custom_dqn_agent.py`
   - Ensure compatibility with `ATC2DEnv`

2. **Test your agent**:
   ```python
   from src.environment import ATC2DEnv
   from src.agents import YourAgent
   
   env = ATC2DEnv()
   agent = YourAgent(env)
   agent.learn(total_timesteps=500000)
   agent.save("models/your_agent.pth")
   ```

3. **Run comparison experiments**:
   - Use `src/evaluation/compare_agents.py`
   - Document results in `results/experiments/`

### For Final Presentation
1. ‚úÖ Complete all agent implementations (3+ minimum)
2. ‚úÖ Run comprehensive hyperparameter experiments
3. ‚úÖ Generate comparison plots and tables
4. ‚úÖ Write analysis and interpretation
5. ‚úÖ Prepare presentation (6-7 minutes)
6. ‚úÖ Upload code and documentation

---

## üìù Development Notes

### Recent Improvements
- **Reward rebalancing** for better learning signal
- **Enhanced UI** with professional graphics
- **Clean project structure** for team collaboration
- **Automated testing** framework
- **Comprehensive logging** for analysis

### Known Issues
- Virtual environment (`rl-env/`) may need recreation
- High variance in early training (expected for DQN)
- Weather effects can occasionally cause outliers

### Future Enhancements
- Multi-agent scenarios (simultaneous controllers)
- Additional weather conditions (fog, ice)
- More complex airspace (3+ runways)
- Real-world data integration

---

## üìö References

- Sutton & Barto - Reinforcement Learning: An Introduction
- Stable-Baselines3 Documentation
- Gymnasium Environment Framework
- DQN Paper: Mnih et al. (2015)

---

## üìû Contact

**Course**: Reinforcement Learning
**Academic Year**: 2025-2026
**Institution**: [Your University]

For questions or contributions, contact team members:
- [Member 1]: [email]
- [Member 2]: [email]
- [Member 3]: [email]

---

## üìÑ License

This project is developed for academic purposes.

---

**Last Updated**: January 14, 2026
